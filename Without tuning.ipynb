{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0798ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression RMSE: 1.00037\n",
      "LinearRegression MAE: 0.78020\n",
      "LinearRegression R-squared: 0.98988\n",
      "LinearRegression SMAPE: 1.18627\n",
      "LinearRegression Training Time: 0.06880 seconds\n",
      "CPU Usage: 15.3 MHz\n",
      "Memory Used: 13015.95703125 MB\n",
      "\n",
      "SVR RMSE: 13.51717\n",
      "SVR MAE: 11.22388\n",
      "SVR R-squared: -0.84728\n",
      "SVR SMAPE: 1.03762\n",
      "SVR Training Time: 0.09827 seconds\n",
      "CPU Usage: 77.9 MHz\n",
      "Memory Used: 13016.34765625 MB\n",
      "\n",
      "AdaBoostRegressor RMSE: 28.25056\n",
      "AdaBoostRegressor MAE: 21.87484\n",
      "AdaBoostRegressor R-squared: -7.06893\n",
      "AdaBoostRegressor SMAPE: 1.16240\n",
      "AdaBoostRegressor Training Time: 0.03089 seconds\n",
      "CPU Usage: 92.3 MHz\n",
      "Memory Used: 13019.69921875 MB\n",
      "\n",
      "RandomForestRegressor RMSE: 12.19346\n",
      "RandomForestRegressor MAE: 7.49687\n",
      "RandomForestRegressor R-squared: -0.50320\n",
      "RandomForestRegressor SMAPE: 1.03598\n",
      "RandomForestRegressor Training Time: 0.69040 seconds\n",
      "CPU Usage: 37.8 MHz\n",
      "Memory Used: 13030.484375 MB\n",
      "\n",
      "GradientBoostingRegressor RMSE: 11.30278\n",
      "GradientBoostingRegressor MAE: 7.01908\n",
      "GradientBoostingRegressor R-squared: -0.29161\n",
      "GradientBoostingRegressor SMAPE: 1.02775\n",
      "GradientBoostingRegressor Training Time: 0.26803 seconds\n",
      "CPU Usage: 45.8 MHz\n",
      "Memory Used: 13033.48046875 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "# data = pd.read_csv('/content/SN_m_tot_V2.0.csv', delimiter=';', header=None)\n",
    "# url = 'https://www.sidc.be/SILSO/INFO/snmtotcsv.php'\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "data = pd.read_csv(url, delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Date', 'Monthly Mean Total Sunspot Number', 'Uncertainty', 'Observations', 'Definitive/Provisional']\n",
    "\n",
    "# Select the 'Monthly Mean Total Sunspot Number' column as the target variable\n",
    "target = data['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the target variable\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#target_scaled = scaler.fit_transform(target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "if X_test.shape[0] == 0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Define the regression models to be evaluated\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    SVR(),\n",
    "    AdaBoostRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor()\n",
    "]\n",
    "\n",
    "# Train and evaluate each model without hyperparameter tuning\n",
    "best_model_predictions = []\n",
    "best_model_smapes = []\n",
    "best_model_mses = []\n",
    "best_model_rmses = []\n",
    "best_model_maes = []\n",
    "best_model_r2s = []\n",
    "training_times = []\n",
    "cpu_usages = []\n",
    "memory_usages = []\n",
    "\n",
    "for model in models:\n",
    "    # Fit the model to the training data\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Get the predictions\n",
    "    predictions = model.predict(X_test).ravel()\n",
    "\n",
    "    # Append the predictions to the list\n",
    "    best_model_predictions.append(predictions)\n",
    "\n",
    "    # Compute evaluation metrics for the model\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    smape_val = smape(y_test, predictions)\n",
    "\n",
    "    best_model_smapes.append(smape_val)\n",
    "    best_model_mses.append(mse)\n",
    "    best_model_rmses.append(rmse)\n",
    "    best_model_maes.append(mae)\n",
    "    best_model_r2s.append(r2)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    model_name = model.__class__.__name__\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    print(f\"{model_name} RMSE: {rmse:.5f}\")\n",
    "    print(f\"{model_name} MAE: {mae:.5f}\")\n",
    "    print(f\"{model_name} R-squared: {r2:.5f}\")\n",
    "    print(f\"{model_name} SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"{model_name} Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {cpu_usage:.1f} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "    # Append training time, CPU usage, and memory usage to lists\n",
    "    training_times.append(training_time)\n",
    "    cpu_usages.append(cpu_usage)\n",
    "    memory_usages.append(psutil.virtual_memory().used / 1024 / 1024)\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(best_model_predictions, axis=0)\n",
    "median_predictions = np.median(best_model_predictions, axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute harmonic mean and geometric mean of the predictions\n",
    "harmonic_mean_predictions = hmean(best_model_predictions)\n",
    "geometric_mean_predictions = gmean(best_model_predictions)\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': [model.__class__.__name__ for model in models] + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': best_model_rmses + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': best_model_maes + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': best_model_r2s + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': best_model_smapes + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': training_times + [None] * 4,\n",
    "    'CPU Usage (MHz)': cpu_usages + [None] * 4,\n",
    "    'Memory Used (MB)': memory_usages + [None] * 4\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('Reg_metrics_test1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cab0898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 5ms/step\n",
      "Model: LSTM\n",
      "RMSE: 4.17160\n",
      "MAE: 3.49857\n",
      "R-squared: 0.82406\n",
      "SMAPE: 101.41411\n",
      "Training Time: 46.67141 seconds\n",
      "CPU Usage: 20.0 MHz\n",
      "Memory Used: 13639.12890625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 7ms/step\n",
      "Model: Stacked LSTM\n",
      "RMSE: 6.00223\n",
      "MAE: 4.87600\n",
      "R-squared: 0.63576\n",
      "SMAPE: 100.85575\n",
      "Training Time: 86.06820 seconds\n",
      "CPU Usage: 0.0 MHz\n",
      "Memory Used: 13662.46875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 10ms/step\n",
      "Model: Bidirectional LSTM\n",
      "RMSE: 4.95809\n",
      "MAE: 3.99177\n",
      "R-squared: 0.75146\n",
      "SMAPE: 101.07885\n",
      "Training Time: 62.46402 seconds\n",
      "CPU Usage: 0.0 MHz\n",
      "Memory Used: 13675.38671875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 9ms/step\n",
      "Model: GRU\n",
      "RMSE: 5.93466\n",
      "MAE: 5.08392\n",
      "R-squared: 0.64392\n",
      "SMAPE: 100.32400\n",
      "Training Time: 46.52151 seconds\n",
      "CPU Usage: 0.0 MHz\n",
      "Memory Used: 13678.203125 MB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Storm\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:275: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(np.array(a, dtype=dtype))\n",
      "C:\\Users\\Storm\\AppData\\Local\\Temp/ipykernel_15340/46033814.py:76: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 200 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
      "C:\\Users\\Storm\\AppData\\Local\\Temp/ipykernel_15340/46033814.py:76: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 200 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "# data = pd.read_csv('/content/SN_m_tot_V2.0.csv', delimiter=';', header=None)\n",
    "# url = 'https://www.sidc.be/SILSO/INFO/snmtotcsv.php'\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "data = pd.read_csv(url, delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Date', 'Monthly Mean Total Sunspot Number', 'Uncertainty', 'Observations', 'Definitive/Provisional']\n",
    "\n",
    "# Select the 'Monthly Mean Total Sunspot Number' column as the target variable\n",
    "target = data['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the target variable\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#target_scaled = scaler.fit_transform(target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 200 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Train and evaluate each deep learning model\n",
    "models = {\n",
    "    'LSTM': create_lstm_model(128),\n",
    "    'Stacked LSTM': create_stacked_lstm_model(128),\n",
    "    'Bidirectional LSTM': create_bidirectional_lstm_model(128),\n",
    "    'GRU': create_gru_model(128)\n",
    "}\n",
    "\n",
    "# Create lists to store evaluation metrics\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "r2_list = []\n",
    "smape_list = []\n",
    "training_times = []\n",
    "cpu_usages = []\n",
    "memory_usages = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                        y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=8,\n",
    "                        verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    smape_val = smape(y_test, predictions)\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = predictions\n",
    "\n",
    "    # Append evaluation metrics to the lists\n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "    smape_list.append(smape_val)\n",
    "    training_times.append(training_time)\n",
    "    cpu_usages.append(psutil.cpu_percent())\n",
    "    memory_usages.append(psutil.virtual_memory().used / 1024 / 1024)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"RMSE: {rmse:.5f}\")\n",
    "    print(f\"MAE: {mae:.5f}\")\n",
    "    print(f\"R-squared: {r2:.5f}\")\n",
    "    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "# Combine the predictions of the models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL_forecasts_test122.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(models.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': rmse_list + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': mae_list + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': r2_list + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': smape_list + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': training_times + [None] * 4,\n",
    "    'CPU Usage (MHz)': cpu_usages + [None] * 4,\n",
    "    'Memory Used (MB)': memory_usages + [None] * 4\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL_metrics_test122.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "091d4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: LSTM\n",
      "RMSE: 4.68633\n",
      "MAE: 3.88201\n",
      "R-squared: 0.77796\n",
      "SMAPE: 101.24834\n",
      "Training Time: 49.80743 seconds\n",
      "CPU Usage: 19.2 MHz\n",
      "Memory Used: 13958.94140625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 0s/step\n",
      "Model: Stacked LSTM\n",
      "RMSE: 5.71059\n",
      "MAE: 4.90557\n",
      "R-squared: 0.67030\n",
      "SMAPE: 100.63671\n",
      "Training Time: 86.05765 seconds\n",
      "CPU Usage: 43.0 MHz\n",
      "Memory Used: 14068.75390625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Bidirectional LSTM\n",
      "RMSE: 5.42839\n",
      "MAE: 4.28589\n",
      "R-squared: 0.70208\n",
      "SMAPE: 100.93393\n",
      "Training Time: 68.58537 seconds\n",
      "CPU Usage: 56.9 MHz\n",
      "Memory Used: 14044.6796875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: GRU\n",
      "RMSE: 6.51305\n",
      "MAE: 4.91354\n",
      "R-squared: 0.57113\n",
      "SMAPE: 101.22677\n",
      "Training Time: 48.87820 seconds\n",
      "CPU Usage: 48.8 MHz\n",
      "Memory Used: 14038.4921875 MB\n",
      "\n",
      "Combined Model:\n",
      "RMSE: 4.43649\n",
      "MAE: 3.56793\n",
      "R-squared: 0.80101\n",
      "SMAPE: 100.99845\n",
      "\n",
      "Median Model:\n",
      "RMSE: 5.27169\n",
      "MAE: 4.15229\n",
      "R-squared: 0.71903\n",
      "SMAPE: 101.01767\n",
      "\n",
      "Harmonic Mean Model:\n",
      "RMSE: 2.03399\n",
      "MAE: 1.62888\n",
      "R-squared: 0.95817\n",
      "SMAPE: 115.00171\n",
      "\n",
      "Geometric Mean Model:\n",
      "RMSE: 2.69274\n",
      "MAE: 2.09890\n",
      "R-squared: 0.92669\n",
      "SMAPE: 111.99373\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Storm\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:275: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(np.array(a, dtype=dtype))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean_rmse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15340/2692881270.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    175\u001b[0m metrics_data = {\n\u001b[0;32m    176\u001b[0m     \u001b[1;34m'Model'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Combined Model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Median Model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Harmonic Mean Model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Geometric Mean Model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[1;34m'RMSE'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_rmse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcombined_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmedian_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mharmonic_mean_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeometric_mean_rmse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;34m'MAE'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_mae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcombined_mae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmedian_mae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mharmonic_mean_mae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeometric_mean_mae\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;34m'R-squared'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_r2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcombined_r2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmedian_r2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mharmonic_mean_r2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeometric_mean_r2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_rmse' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean, gmean\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "data = pd.read_csv(url, delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Date', 'Monthly Mean Total Sunspot Number', 'Uncertainty', 'Observations', 'Definitive/Provisional']\n",
    "\n",
    "# Select the 'Monthly Mean Total Sunspot Number' column as the target variable\n",
    "target = data['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    epsilon = 1e-10\n",
    "    percentage_error = 200 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + epsilon)\n",
    "    return np.mean(percentage_error)\n",
    "\n",
    "# ... (the rest of the code remains unchanged) ...\n",
    "\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                        y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=8,\n",
    "                        verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    smape_val = smape(y_test, predictions)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"RMSE: {rmse:.5f}\")\n",
    "    print(f\"MAE: {mae:.5f}\")\n",
    "    print(f\"R-squared: {r2:.5f}\")\n",
    "    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "    return predictions, rmse, mae, r2, smape_val\n",
    "\n",
    "# Create models\n",
    "models = {\n",
    "    'LSTM': create_lstm_model(128),\n",
    "    'Stacked LSTM': create_stacked_lstm_model(128),\n",
    "    'Bidirectional LSTM': create_bidirectional_lstm_model(128),\n",
    "    'GRU': create_gru_model(128)\n",
    "}\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Train and evaluate each deep learning model\n",
    "for model_name, model in models.items():\n",
    "    predictions, rmse, mae, r2, smape_val = train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = predictions\n",
    "\n",
    "# Combine the predictions of the models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Print evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "print(\"Combined Model:\")\n",
    "print(f\"RMSE: {combined_rmse:.5f}\")\n",
    "print(f\"MAE: {combined_mae:.5f}\")\n",
    "print(f\"R-squared: {combined_r2:.5f}\")\n",
    "print(f\"SMAPE: {combined_smape:.5f}\")\n",
    "print()\n",
    "\n",
    "# Print evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "print(\"Median Model:\")\n",
    "print(f\"RMSE: {median_rmse:.5f}\")\n",
    "print(f\"MAE: {median_mae:.5f}\")\n",
    "print(f\"R-squared: {median_r2:.5f}\")\n",
    "print(f\"SMAPE: {median_smape:.5f}\")\n",
    "print()\n",
    "\n",
    "# Print evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "print(\"Harmonic Mean Model:\")\n",
    "print(f\"RMSE: {harmonic_mean_rmse:.5f}\")\n",
    "print(f\"MAE: {harmonic_mean_mae:.5f}\")\n",
    "print(f\"R-squared: {harmonic_mean_r2:.5f}\")\n",
    "print(f\"SMAPE: {harmonic_mean_smape:.5f}\")\n",
    "print()\n",
    "\n",
    "# Print evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "print(\"Geometric Mean Model:\")\n",
    "print(f\"RMSE: {geometric_mean_rmse:.5f}\")\n",
    "print(f\"MAE: {geometric_mean_mae:.5f}\")\n",
    "print(f\"R-squared: {geometric_mean_r2:.5f}\")\n",
    "print(f\"SMAPE: {geometric_mean_smape:.5f}\")\n",
    "print()\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(models.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': list(mean_rmse.values()) + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': list(mean_mae.values()) + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': list(mean_r2.values()) + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': list(mean_smape.values()) + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6da35784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: LSTM\n",
      "RMSE: 4.92054\n",
      "MAE: 4.07627\n",
      "R-squared: 0.75521\n",
      "SMAPE: 100.82266\n",
      "Training Time: 33.46689 seconds\n",
      "CPU Usage: 15.6 MHz\n",
      "Memory Used: 13896.890625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Bidirectional LSTM\n",
      "RMSE: 5.79108\n",
      "MAE: 4.49262\n",
      "R-squared: 0.66094\n",
      "SMAPE: 100.98292\n",
      "Training Time: 46.30838 seconds\n",
      "CPU Usage: 48.1 MHz\n",
      "Memory Used: 13968.41015625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: GRU\n",
      "RMSE: 5.67785\n",
      "MAE: 4.51462\n",
      "R-squared: 0.67407\n",
      "SMAPE: 101.18125\n",
      "Training Time: 32.36674 seconds\n",
      "CPU Usage: 39.7 MHz\n",
      "Memory Used: 13980.62109375 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "data = pd.read_csv(url, delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Date', 'Monthly Mean Total Sunspot Number', 'Uncertainty', 'Observations', 'Definitive/Provisional']\n",
    "\n",
    "# Select the 'Monthly Mean Total Sunspot Number' column as the target variable\n",
    "target = data['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Create deep learning models\n",
    "def create_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 200 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                        y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=8,\n",
    "                        verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    smape_val = smape(y_test, predictions)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"RMSE: {rmse:.5f}\")\n",
    "    print(f\"MAE: {mae:.5f}\")\n",
    "    print(f\"R-squared: {r2:.5f}\")\n",
    "    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "    return predictions, rmse, mae, r2, smape_val\n",
    "\n",
    "# Create models\n",
    "models = {\n",
    "    'LSTM': create_lstm_model(128),\n",
    "    'Bidirectional LSTM': create_bidirectional_lstm_model(128),\n",
    "    'GRU': create_gru_model(128)\n",
    "}\n",
    "\n",
    "# Create lists to store evaluation metrics\n",
    "best_model_predictions = {}\n",
    "best_model_rmses = {}\n",
    "best_model_maes = {}\n",
    "best_model_r2s = {}\n",
    "best_model_smapes = {}\n",
    "\n",
    "# Train and evaluate each deep learning model\n",
    "for model_name, model in models.items():\n",
    "    predictions, rmse, mae, r2, smape_val = train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    best_model_predictions[model_name] = predictions\n",
    "    best_model_rmses[model_name] = rmse\n",
    "    best_model_maes[model_name] = mae\n",
    "    best_model_r2s[model_name] = r2\n",
    "    best_model_smapes[model_name] = smape_val\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(best_model_predictions.values()), axis=0)\n",
    "median_predictions = np.median(list(best_model_predictions.values()), axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute harmonic mean and geometric mean of the predictions\n",
    "harmonic_mean_predictions = hmean(list(best_model_predictions.values()))\n",
    "geometric_mean_predictions = gmean(list(best_model_predictions.values()))\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(models.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': list(best_model_rmses.values()) + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': list(best_model_maes.values()) + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': list(best_model_r2s.values()) + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': list(best_model_smapes.values()) + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape]\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the evaluation metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df.to_csv('deep_learning_model_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a749359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM\n",
      "RMSE: 4.41828\n",
      "MAE: 3.71629\n",
      "R-squared: 0.80264\n",
      "SMAPE: 101.12625\n",
      "Training Time: 35.26569 seconds\n",
      "CPU Usage: 21.0 MHz\n",
      "Memory Used: 14295.28125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 7ms/step\n",
      "Model: Stacked LSTM\n",
      "RMSE: 6.81194\n",
      "MAE: 5.26734\n",
      "R-squared: 0.53086\n",
      "SMAPE: 101.59699\n",
      "Training Time: 57.34942 seconds\n",
      "CPU Usage: 49.0 MHz\n",
      "Memory Used: 14362.44140625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM\n",
      "RMSE: 4.97584\n",
      "MAE: 4.25540\n",
      "R-squared: 0.74968\n",
      "SMAPE: 100.50771\n",
      "Training Time: 42.84408 seconds\n",
      "CPU Usage: 53.2 MHz\n",
      "Memory Used: 14378.984375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "Model: GRU\n",
      "RMSE: 4.93938\n",
      "MAE: 4.03686\n",
      "R-squared: 0.75334\n",
      "SMAPE: 100.92689\n",
      "Training Time: 30.35164 seconds\n",
      "CPU Usage: 34.3 MHz\n",
      "Memory Used: 14461.7109375 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "data = pd.read_csv(url, delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Date', 'Monthly Mean Total Sunspot Number', 'Uncertainty', 'Observations', 'Definitive/Provisional']\n",
    "\n",
    "# Select the 'Monthly Mean Total Sunspot Number' column as the target variable\n",
    "target = data['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Create deep learning models\n",
    "def create_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 200 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                        y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=8,\n",
    "                        verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    smape_val = smape(y_test, predictions)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"RMSE: {rmse:.5f}\")\n",
    "    print(f\"MAE: {mae:.5f}\")\n",
    "    print(f\"R-squared: {r2:.5f}\")\n",
    "    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "    return predictions, rmse, mae, r2, smape_val\n",
    "\n",
    "# Create models\n",
    "models = {\n",
    "    'LSTM': create_lstm_model(128),\n",
    "    'Stacked LSTM': create_stacked_lstm_model(128),\n",
    "    'Bidirectional LSTM': create_bidirectional_lstm_model(128),\n",
    "    'GRU': create_gru_model(128)\n",
    "}\n",
    "\n",
    "# Create lists to store evaluation metrics\n",
    "best_model_predictions = {}\n",
    "best_model_rmses = {}\n",
    "best_model_maes = {}\n",
    "best_model_r2s = {}\n",
    "best_model_smapes = {}\n",
    "\n",
    "# Train and evaluate each deep learning model\n",
    "for model_name, model in models.items():\n",
    "    predictions, rmse, mae, r2, smape_val = train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    best_model_predictions[model_name] = predictions\n",
    "    best_model_rmses[model_name] = rmse\n",
    "    best_model_maes[model_name] = mae\n",
    "    best_model_r2s[model_name] = r2\n",
    "    best_model_smapes[model_name] = smape_val\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(best_model_predictions.values()), axis=0)\n",
    "median_predictions = np.median(list(best_model_predictions.values()), axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute harmonic mean and geometric mean of the predictions\n",
    "harmonic_mean_predictions = hmean(list(best_model_predictions.values()))\n",
    "geometric_mean_predictions = gmean(list(best_model_predictions.values()))\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(models.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': list(best_model_rmses.values()) + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': list(best_model_maes.values()) + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': list(best_model_r2s.values()) + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': list(best_model_smapes.values()) + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape]\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the evaluation metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df.to_csv('deep_learning_model_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cfdfee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: LSTM\n",
      "RMSE: 4.66370\n",
      "MAE: 3.86392\n",
      "R-squared: 0.78010\n",
      "SMAPE: 101.45635\n",
      "Training Time: 35.35311 seconds\n",
      "CPU Usage: 19.5 MHz\n",
      "Memory Used: 14413.2 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM\n",
      "RMSE: 5.99799\n",
      "MAE: 4.84093\n",
      "R-squared: 0.63627\n",
      "SMAPE: 101.12073\n",
      "Training Time: 60.15337 seconds\n",
      "CPU Usage: 43.3 MHz\n",
      "Memory Used: 14440.5 MB\n",
      "\n",
      "3/3 [==============================] - 1s 5ms/step\n",
      "Model: Bidirectional LSTM\n",
      "RMSE: 5.14083\n",
      "MAE: 4.13064\n",
      "R-squared: 0.73280\n",
      "SMAPE: 101.16717\n",
      "Training Time: 43.53306 seconds\n",
      "CPU Usage: 46.8 MHz\n",
      "Memory Used: 14188.6 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: GRU\n",
      "RMSE: 6.99919\n",
      "MAE: 5.71987\n",
      "R-squared: 0.50471\n",
      "SMAPE: 100.52319\n",
      "Training Time: 30.55079 seconds\n",
      "CPU Usage: 31.0 MHz\n",
      "Memory Used: 14173.6 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "data = pd.read_csv(url, delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Date', 'Monthly Mean Total Sunspot Number', 'Uncertainty', 'Observations', 'Definitive/Provisional']\n",
    "\n",
    "# Select the 'Monthly Mean Total Sunspot Number' column as the target variable\n",
    "target = data['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Create deep learning models\n",
    "def create_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 200 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                        y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=8,\n",
    "                        verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    smape_val = smape(y_test, predictions)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"RMSE: {rmse:.5f}\")\n",
    "    print(f\"MAE: {mae:.5f}\")\n",
    "    print(f\"R-squared: {r2:.5f}\")\n",
    "    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_usage = psutil.virtual_memory().used / 1024 / 1024\n",
    "    print(f\"CPU Usage: {cpu_usage:.1f} MHz\")\n",
    "    print(f\"Memory Used: {memory_usage:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "    return predictions, rmse, mae, r2, smape_val, training_time, cpu_usage, memory_usage\n",
    "\n",
    "# Create models\n",
    "models = {\n",
    "    'LSTM': create_lstm_model(128),\n",
    "    'Stacked LSTM': create_stacked_lstm_model(128),\n",
    "    'Bidirectional LSTM': create_bidirectional_lstm_model(128),\n",
    "    'GRU': create_gru_model(128)\n",
    "}\n",
    "\n",
    "# Create lists to store evaluation metrics\n",
    "best_model_predictions = {}\n",
    "best_model_rmses = {}\n",
    "best_model_maes = {}\n",
    "best_model_r2s = {}\n",
    "best_model_smapes = {}\n",
    "training_times = {}\n",
    "cpu_usages = {}\n",
    "memory_usages = {}\n",
    "\n",
    "# Train and evaluate each deep learning model\n",
    "for model_name, model in models.items():\n",
    "    predictions, rmse, mae, r2, smape_val, training_time, cpu_usage, memory_usage = train_and_evaluate_model(\n",
    "        model, model_name, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    best_model_predictions[model_name] = predictions\n",
    "    best_model_rmses[model_name] = rmse\n",
    "    best_model_maes[model_name] = mae\n",
    "    best_model_r2s[model_name] = r2\n",
    "    best_model_smapes[model_name] = smape_val\n",
    "    training_times[model_name] = training_time\n",
    "    cpu_usages[model_name] = cpu_usage\n",
    "    memory_usages[model_name] = memory_usage\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(best_model_predictions.values()), axis=0)\n",
    "median_predictions = np.median(list(best_model_predictions.values()), axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute harmonic mean and geometric mean of the predictions\n",
    "harmonic_mean_predictions = hmean(list(best_model_predictions.values()))\n",
    "geometric_mean_predictions = gmean(list(best_model_predictions.values()))\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(models.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': list(best_model_rmses.values()) + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': list(best_model_maes.values()) + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': list(best_model_r2s.values()) + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': list(best_model_smapes.values()) + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': list(training_times.values()) + [None] * 4,\n",
    "    'CPU Usage (MHz)': list(cpu_usages.values()) + [None] * 4,\n",
    "    'Memory Used (MB)': list(memory_usages.values()) + [None] * 4\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the evaluation metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df.to_csv('deep_learning_model_metrics2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00ab3caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: LSTM\n",
      "RMSE: 1.11740\n",
      "MAE: 0.85287\n",
      "R-squared: 0.98738\n",
      "SMAPE: 42.76431\n",
      "Training Time: 32.64415 seconds\n",
      "CPU Usage: 20.0 MHz\n",
      "Memory Used: 14499.09765625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 7ms/step\n",
      "Model: Stacked LSTM\n",
      "RMSE: 6.28162\n",
      "MAE: 6.16529\n",
      "R-squared: 0.60106\n",
      "SMAPE: 86.28563\n",
      "Training Time: 57.05143 seconds\n",
      "CPU Usage: 42.2 MHz\n",
      "Memory Used: 14532.328125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Bidirectional LSTM\n",
      "RMSE: 2.10087\n",
      "MAE: 1.90499\n",
      "R-squared: 0.95538\n",
      "SMAPE: 59.81345\n",
      "Training Time: 42.91935 seconds\n",
      "CPU Usage: 45.9 MHz\n",
      "Memory Used: 14587.0625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 9ms/step\n",
      "Model: GRU\n",
      "RMSE: 4.51402\n",
      "MAE: 4.33633\n",
      "R-squared: 0.79399\n",
      "SMAPE: 77.31022\n",
      "Training Time: 30.06283 seconds\n",
      "CPU Usage: 39.9 MHz\n",
      "Memory Used: 14545.7578125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 0s/step\n",
      "3/3 [==============================] - 0s 0s/step\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 0s/step\n",
      "3/3 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test_original_scale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15340/296168462.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;31m# Compute evaluation metrics for the combined model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[0mcombined_predictions_original_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m \u001b[0mcombined_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_original_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombined_predictions_original_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[0mcombined_rmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[0mcombined_mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_original_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombined_predictions_original_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test_original_scale' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "# data = pd.read_csv('/content/SN_m_tot_V2.0.csv', delimiter=';', header=None)\n",
    "# url = 'https://www.sidc.be/SILSO/INFO/snmtotcsv.php'\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "data = pd.read_csv(url, delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Date', 'Monthly Mean Total Sunspot Number', 'Uncertainty', 'Observations', 'Definitive/Provisional']\n",
    "\n",
    "# Select the 'Monthly Mean Total Sunspot Number' column as the target variable\n",
    "target = data['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the target variable\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaled = scaler.fit_transform(target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target_scaled[:train_size]\n",
    "test_data = target_scaled[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "if X_test.shape[0] == 0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Compute SMAPE on the original scale\n",
    "def smape(y_true, y_pred):\n",
    "    return 200 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create deep learning models\n",
    "def create_lstm_model(units):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(units))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                        y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=8,\n",
    "                        verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "    predictions_original_scale = scaler.inverse_transform(predictions.reshape(-1, 1)).ravel()\n",
    "    y_test_original_scale = scaler.inverse_transform(y_test).ravel()\n",
    "\n",
    "    mse = mean_squared_error(y_test_original_scale, predictions_original_scale)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_original_scale, predictions_original_scale)\n",
    "    r2 = r2_score(y_test_original_scale, predictions_original_scale)\n",
    "    smape_val = smape(y_test_original_scale, predictions_original_scale)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"RMSE: {rmse:.5f}\")\n",
    "    print(f\"MAE: {mae:.5f}\")\n",
    "    print(f\"R-squared: {r2:.5f}\")\n",
    "    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "    return predictions, rmse, mae, r2, smape_val\n",
    "\n",
    "# Create models\n",
    "models = {\n",
    "    'LSTM': create_lstm_model(128),\n",
    "    'Stacked LSTM': create_stacked_lstm_model(128),\n",
    "    'Bidirectional LSTM': create_bidirectional_lstm_model(128),\n",
    "    'GRU': create_gru_model(128)\n",
    "}\n",
    "\n",
    "# Create lists to store evaluation metrics\n",
    "model_names = []\n",
    "model_rmse = []\n",
    "model_mae = []\n",
    "model_r2 = []\n",
    "model_smape = []\n",
    "training_times = []\n",
    "cpu_usages = []\n",
    "memory_usages = []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model_names.append(model_name)\n",
    "    predictions, rmse, mae, r2, smape_val = train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test)\n",
    "    model_rmse.append(rmse)\n",
    "    model_mae.append(mae)\n",
    "    model_r2.append(r2)\n",
    "    model_smape.append(smape_val)\n",
    "\n",
    "    # Append training time, CPU usage, and memory usage to lists\n",
    "    training_times.append(time.time() - start_time)\n",
    "    cpu_usages.append(psutil.cpu_percent())\n",
    "    memory_usages.append(psutil.virtual_memory().used / 1024 / 1024)\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean([model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel() for model in models.values()], axis=0)\n",
    "median_predictions = np.median([model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel() for model in models.values()], axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_predictions_original_scale = scaler.inverse_transform(combined_predictions.reshape(-1, 1)).ravel()\n",
    "combined_mse = mean_squared_error(y_test_original_scale, combined_predictions_original_scale)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test_original_scale, combined_predictions_original_scale)\n",
    "combined_r2 = r2_score(y_test_original_scale, combined_predictions_original_scale)\n",
    "combined_smape = smape(y_test_original_scale, combined_predictions_original_scale)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_predictions_original_scale = scaler.inverse_transform(median_predictions.reshape(-1, 1)).ravel()\n",
    "median_mse = mean_squared_error(y_test_original_scale, median_predictions_original_scale)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test_original_scale, median_predictions_original_scale)\n",
    "median_r2 = r2_score(y_test_original_scale, median_predictions_original_scale)\n",
    "median_smape = smape(y_test_original_scale, median_predictions_original_scale)\n",
    "\n",
    "# Compute harmonic mean and geometric mean of the predictions\n",
    "best_model_predictions = [model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel() for model in models.values()]\n",
    "harmonic_mean_predictions = hmean(best_model_predictions)\n",
    "geometric_mean_predictions = gmean(best_model_predictions)\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_predictions_original_scale = scaler.inverse_transform(harmonic_mean_predictions.reshape(-1, 1)).ravel()\n",
    "harmonic_mean_mse = mean_squared_error(y_test_original_scale, harmonic_mean_predictions_original_scale)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test_original_scale, harmonic_mean_predictions_original_scale)\n",
    "harmonic_mean_r2 = r2_score(y_test_original_scale, harmonic_mean_predictions_original_scale)\n",
    "harmonic_mean_smape = smape(y_test_original_scale, harmonic_mean_predictions_original_scale)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_predictions_original_scale = scaler.inverse_transform(geometric_mean_predictions.reshape(-1, 1)).ravel()\n",
    "geometric_mean_mse = mean_squared_error(y_test_original_scale, geometric_mean_predictions_original_scale)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test_original_scale, geometric_mean_predictions_original_scale)\n",
    "geometric_mean_r2 = r2_score(y_test_original_scale, geometric_mean_predictions_original_scale)\n",
    "geometric_mean_smape = smape(y_test_original_scale, geometric_mean_predictions_original_scale)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': model_names + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': model_rmse + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': model_mae + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': model_r2 + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': model_smape + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': training_times + [None] * 4,\n",
    "    'CPU Usage (MHz)': cpu_usages + [None] * 4,\n",
    "    'Memory Used (MB)': memory_usages + [None] * 4\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('deep_learning_model_metrics3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f48d58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 0s/step\n",
      "Model: LSTM\n",
      "RMSE: 4.56139\n",
      "MAE: 3.89048\n",
      "R-squared: 0.78964\n",
      "SMAPE: 100.92687\n",
      "Training Time: 34.64167 seconds\n",
      "CPU Usage: 21.1 MHz\n",
      "Memory Used: 14526.9140625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: Bidirectional LSTM\n",
      "RMSE: 5.52531\n",
      "MAE: 4.52270\n",
      "R-squared: 0.69134\n",
      "SMAPE: 100.65072\n",
      "Training Time: 44.65468 seconds\n",
      "CPU Usage: 41.8 MHz\n",
      "Memory Used: 14445.921875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: GRU\n",
      "RMSE: 5.66491\n",
      "MAE: 4.51402\n",
      "R-squared: 0.67555\n",
      "SMAPE: 101.46372\n",
      "Training Time: 33.19287 seconds\n",
      "CPU Usage: 38.6 MHz\n",
      "Memory Used: 14462.1328125 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "data = pd.read_csv(url, delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Date', 'Monthly Mean Total Sunspot Number', 'Uncertainty', 'Observations', 'Definitive/Provisional']\n",
    "\n",
    "# Select the 'Monthly Mean Total Sunspot Number' column as the target variable\n",
    "target = data['Monthly Mean Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Create deep learning models\n",
    "def create_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 200 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                        y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=8,\n",
    "                        verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    smape_val = smape(y_test, predictions)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"RMSE: {rmse:.5f}\")\n",
    "    print(f\"MAE: {mae:.5f}\")\n",
    "    print(f\"R-squared: {r2:.5f}\")\n",
    "    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "    return predictions, rmse, mae, r2, smape_val\n",
    "\n",
    "# Create models\n",
    "models = {\n",
    "    'LSTM': create_lstm_model(128),\n",
    "    'Bidirectional LSTM': create_bidirectional_lstm_model(128),\n",
    "    'GRU': create_gru_model(128)\n",
    "}\n",
    "\n",
    "# Create lists to store evaluation metrics\n",
    "best_model_predictions = {}\n",
    "best_model_rmses = {}\n",
    "best_model_maes = {}\n",
    "best_model_r2s = {}\n",
    "best_model_smapes = {}\n",
    "\n",
    "# Train and evaluate each deep learning model\n",
    "for model_name, model in models.items():\n",
    "    predictions, rmse, mae, r2, smape_val = train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    best_model_predictions[model_name] = predictions\n",
    "    best_model_rmses[model_name] = rmse\n",
    "    best_model_maes[model_name] = mae\n",
    "    best_model_r2s[model_name] = r2\n",
    "    best_model_smapes[model_name] = smape_val\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(best_model_predictions.values()), axis=0)\n",
    "median_predictions = np.median(list(best_model_predictions.values()), axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute harmonic mean and geometric mean of the predictions\n",
    "harmonic_mean_predictions = hmean(list(best_model_predictions.values()))\n",
    "geometric_mean_predictions = gmean(list(best_model_predictions.values()))\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(models.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': list(best_model_rmses.values()) + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': list(best_model_maes.values()) + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': list(best_model_r2s.values()) + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': list(best_model_smapes.values()) + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape]\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the evaluation metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df.to_csv('deep_learning_model_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af3b568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
