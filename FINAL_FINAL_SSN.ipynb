{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b9588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step\n",
      "LSTM (Units: 8, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 95.79604\n",
      "MAE: 86.35425\n",
      "R-squared: -3.82686\n",
      "SMAPE: 0.86176\n",
      "Training Time: 28.85538 seconds\n",
      "CPU Usage: 21.1 MHz\n",
      "Memory Used: 9023.625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "LSTM (Units: 8, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 12)\n",
      "RMSE: 105.85124\n",
      "MAE: 96.80366\n",
      "R-squared: -4.89334\n",
      "SMAPE: 1.01179\n",
      "Training Time: 19.44170 seconds\n",
      "CPU Usage: 27.6 MHz\n",
      "Memory Used: 9038.45703125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "LSTM (Units: 32, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 38.39452\n",
      "MAE: 29.46473\n",
      "R-squared: 0.22463\n",
      "SMAPE: 0.40429\n",
      "Training Time: 25.93659 seconds\n",
      "CPU Usage: 28.5 MHz\n",
      "Memory Used: 9142.171875 MB\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FC6EBDF9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "LSTM (Units: 32, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 12)\n",
      "RMSE: 56.26533\n",
      "MAE: 45.74767\n",
      "R-squared: -0.66514\n",
      "SMAPE: 0.47421\n",
      "Training Time: 17.22245 seconds\n",
      "CPU Usage: 30.0 MHz\n",
      "Memory Used: 9162.7265625 MB\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FC6EBDF8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "LSTM (Units: 64, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 21.00673\n",
      "MAE: 16.10303\n",
      "R-squared: 0.76789\n",
      "SMAPE: 0.39188\n",
      "Training Time: 22.19951 seconds\n",
      "CPU Usage: 31.5 MHz\n",
      "Memory Used: 9139.86328125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "LSTM (Units: 64, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 12)\n",
      "RMSE: 36.21402\n",
      "MAE: 27.59508\n",
      "R-squared: 0.31020\n",
      "SMAPE: 0.39882\n",
      "Training Time: 15.43390 seconds\n",
      "CPU Usage: 36.4 MHz\n",
      "Memory Used: 9173.94921875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 14.77513\n",
      "MAE: 11.76202\n",
      "R-squared: 0.88518\n",
      "SMAPE: 0.39817\n",
      "Training Time: 28.15722 seconds\n",
      "CPU Usage: 32.2 MHz\n",
      "Memory Used: 9204.60546875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 12)\n",
      "RMSE: 21.63870\n",
      "MAE: 16.72314\n",
      "R-squared: 0.75372\n",
      "SMAPE: 0.39413\n",
      "Training Time: 18.68861 seconds\n",
      "CPU Usage: 35.3 MHz\n",
      "Memory Used: 9206.31640625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 11.08562\n",
      "MAE: 9.11584\n",
      "R-squared: 0.93536\n",
      "SMAPE: 0.39726\n",
      "Training Time: 59.67121 seconds\n",
      "CPU Usage: 62.4 MHz\n",
      "Memory Used: 9207.6484375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 12)\n",
      "RMSE: 13.49713\n",
      "MAE: 10.78417\n",
      "R-squared: 0.90418\n",
      "SMAPE: 0.39481\n",
      "Training Time: 45.61956 seconds\n",
      "CPU Usage: 58.6 MHz\n",
      "Memory Used: 9231.640625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "True Values and Forecasts:\n",
      "    True Value  True Value  \\\n",
      "0         32.6        32.6   \n",
      "1         36.7        36.7   \n",
      "2         42.7        42.7   \n",
      "3         49.8        49.8   \n",
      "4         56.9        56.9   \n",
      "..         ...         ...   \n",
      "66       111.3       111.3   \n",
      "67       107.7       107.7   \n",
      "68       103.2       103.2   \n",
      "69        99.5        99.5   \n",
      "70        96.1        96.1   \n",
      "\n",
      "    LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)  \n",
      "0                                           28.380564                          \n",
      "1                                           31.514772                          \n",
      "2                                           35.396511                          \n",
      "3                                           41.069366                          \n",
      "4                                           47.806602                          \n",
      "..                                                ...                          \n",
      "66                                         112.679100                          \n",
      "67                                         111.320114                          \n",
      "68                                         107.520370                          \n",
      "69                                         102.710678                          \n",
      "70                                          98.722717                          \n",
      "\n",
      "[71 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for LSTM model\n",
    "hyperparameters = {\n",
    "    'LSTM': {\n",
    "        'units': [8, 32, 64, 128, 256],\n",
    "        'dropout_rate': [0.0],\n",
    "        'learning_rate': [0.001],\n",
    "        'batch_size': [8, 12]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate the LSTM model with hyperparameter tuning\n",
    "best_rmse = np.inf\n",
    "best_model = None\n",
    "best_units = 0\n",
    "best_dropout_rate = 0\n",
    "best_learning_rate = 0\n",
    "best_batch_size = 0\n",
    "\n",
    "for units in hyperparameters['LSTM']['units']:\n",
    "    for dropout_rate in hyperparameters['LSTM']['dropout_rate']:\n",
    "        for learning_rate in hyperparameters['LSTM']['learning_rate']:\n",
    "            for batch_size in hyperparameters['LSTM']['batch_size']:\n",
    "                model = create_lstm_model(units, dropout_rate)\n",
    "                model.optimizer.lr = learning_rate\n",
    "\n",
    "                start_time = time.time()\n",
    "                history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                    y_train,\n",
    "                                    epochs=100,\n",
    "                                    batch_size=batch_size,\n",
    "                                    verbose=0)\n",
    "                training_time = time.time() - start_time\n",
    "\n",
    "                predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                mse = mean_squared_error(y_test, predictions)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(y_test, predictions)\n",
    "                r2 = r2_score(y_test, predictions)\n",
    "                smape_val = smape(y_test, predictions)\n",
    "\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = model\n",
    "                    best_units = units\n",
    "                    best_dropout_rate = dropout_rate\n",
    "                    best_learning_rate = learning_rate\n",
    "                    best_batch_size = batch_size\n",
    "\n",
    "                print(f\"LSTM (Units: {units}, Dropout Rate: {dropout_rate}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                print(f\"RMSE: {rmse:.5f}\")\n",
    "                print(f\"MAE: {mae:.5f}\")\n",
    "                print(f\"R-squared: {r2:.5f}\")\n",
    "                print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                print()\n",
    "\n",
    "# Store the predictions in the dictionary\n",
    "model_data[f\"LSTM (Units: {best_units}, Dropout Rate: {best_dropout_rate}, Learning Rate: {best_learning_rate}, Batch Size: {best_batch_size})\"] = best_model.predict(\n",
    "    X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "# Include evaluation metrics and best hyperparameters in the eval metrics dictionary\n",
    "eval_metrics['LSTM'] = {\n",
    "    'Best Units': best_units,\n",
    "    'Best Dropout Rate': best_dropout_rate,\n",
    "    'Best Learning Rate': best_learning_rate,\n",
    "    'Best Batch Size': best_batch_size,\n",
    "    'RMSE': best_rmse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2,\n",
    "    'SMAPE': smape_val,\n",
    "    'Training Time (seconds)': training_time,\n",
    "    'CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "    'Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024\n",
    "}\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('lstm_forecasts_1875_to_1941_792interval.csv', index=False)\n",
    "\n",
    "# Save evaluation metrics to a CSV file\n",
    "df_metrics = pd.DataFrame(eval_metrics).T\n",
    "df_metrics.to_csv('lstm_eval_metrics_1875_to_1941_792interval.csv')\n",
    "\n",
    "# Print true values and forecasts in table format\n",
    "df_true_values = pd.DataFrame(y_test, columns=['True Value'])\n",
    "df_forecasts = pd.DataFrame(model_data)\n",
    "\n",
    "print(\"True Values and Forecasts:\")\n",
    "print(pd.concat([df_true_values, df_forecasts], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e4aed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 6ms/step\n",
      "LSTM (Units: 8, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 96.08934\n",
      "MAE: 86.67450\n",
      "R-squared: -3.85646\n",
      "SMAPE: 0.86601\n",
      "Training Time: 24.78710 seconds\n",
      "CPU Usage: 24.2 MHz\n",
      "Memory Used: 9306.3203125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "LSTM (Units: 8, Dropout Rate: 0.0, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 30.65566\n",
      "MAE: 22.76835\n",
      "R-squared: 0.50570\n",
      "SMAPE: 0.39251\n",
      "Training Time: 28.21861 seconds\n",
      "CPU Usage: 35.4 MHz\n",
      "Memory Used: 9259.67578125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "LSTM (Units: 8, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 93.76140\n",
      "MAE: 84.08180\n",
      "R-squared: -3.62400\n",
      "SMAPE: 0.83151\n",
      "Training Time: 27.16693 seconds\n",
      "CPU Usage: 35.7 MHz\n",
      "Memory Used: 9267.02734375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 0s/step\n",
      "LSTM (Units: 8, Dropout Rate: 0.2, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 36.84452\n",
      "MAE: 28.99686\n",
      "R-squared: 0.28597\n",
      "SMAPE: 0.41003\n",
      "Training Time: 28.65410 seconds\n",
      "CPU Usage: 33.0 MHz\n",
      "Memory Used: 9329.703125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "LSTM (Units: 32, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 37.11263\n",
      "MAE: 28.29915\n",
      "R-squared: 0.27554\n",
      "SMAPE: 0.39801\n",
      "Training Time: 24.69938 seconds\n",
      "CPU Usage: 32.3 MHz\n",
      "Memory Used: 9333.51953125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "LSTM (Units: 32, Dropout Rate: 0.0, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 22.59907\n",
      "MAE: 18.33055\n",
      "R-squared: 0.73137\n",
      "SMAPE: 0.40311\n",
      "Training Time: 23.39726 seconds\n",
      "CPU Usage: 45.5 MHz\n",
      "Memory Used: 9347.8203125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "LSTM (Units: 32, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 38.26412\n",
      "MAE: 29.59441\n",
      "R-squared: 0.22989\n",
      "SMAPE: 0.40693\n",
      "Training Time: 24.75397 seconds\n",
      "CPU Usage: 35.1 MHz\n",
      "Memory Used: 9352.27734375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "LSTM (Units: 32, Dropout Rate: 0.2, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 17.78838\n",
      "MAE: 13.83341\n",
      "R-squared: 0.83357\n",
      "SMAPE: 0.38577\n",
      "Training Time: 24.31713 seconds\n",
      "CPU Usage: 38.4 MHz\n",
      "Memory Used: 9355.203125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "LSTM (Units: 64, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 21.72848\n",
      "MAE: 16.42084\n",
      "R-squared: 0.75167\n",
      "SMAPE: 0.38410\n",
      "Training Time: 23.07802 seconds\n",
      "CPU Usage: 41.4 MHz\n",
      "Memory Used: 9393.01171875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 9ms/step\n",
      "LSTM (Units: 64, Dropout Rate: 0.0, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 10.40777\n",
      "MAE: 8.15122\n",
      "R-squared: 0.94302\n",
      "SMAPE: 0.38659\n",
      "Training Time: 23.51275 seconds\n",
      "CPU Usage: 39.2 MHz\n",
      "Memory Used: 9401.734375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "LSTM (Units: 64, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 24.42160\n",
      "MAE: 18.58338\n",
      "R-squared: 0.68630\n",
      "SMAPE: 0.38979\n",
      "Training Time: 23.21063 seconds\n",
      "CPU Usage: 35.2 MHz\n",
      "Memory Used: 9402.6875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "LSTM (Units: 64, Dropout Rate: 0.2, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 19.11408\n",
      "MAE: 14.82902\n",
      "R-squared: 0.80783\n",
      "SMAPE: 0.38453\n",
      "Training Time: 24.23980 seconds\n",
      "CPU Usage: 42.3 MHz\n",
      "Memory Used: 9432.78515625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 13.13657\n",
      "MAE: 10.21529\n",
      "R-squared: 0.90923\n",
      "SMAPE: 0.38630\n",
      "Training Time: 28.36397 seconds\n",
      "CPU Usage: 38.4 MHz\n",
      "Memory Used: 9448.421875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 10.07378\n",
      "MAE: 8.03703\n",
      "R-squared: 0.94662\n",
      "SMAPE: 0.38815\n",
      "Training Time: 28.63639 seconds\n",
      "CPU Usage: 34.4 MHz\n",
      "Memory Used: 9536.5234375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "LSTM (Units: 128, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 15.61898\n",
      "MAE: 12.19064\n",
      "R-squared: 0.87169\n",
      "SMAPE: 0.38822\n",
      "Training Time: 30.48329 seconds\n",
      "CPU Usage: 34.1 MHz\n",
      "Memory Used: 9544.21875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "LSTM (Units: 128, Dropout Rate: 0.2, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 13.53451\n",
      "MAE: 10.75460\n",
      "R-squared: 0.90365\n",
      "SMAPE: 0.38789\n",
      "Training Time: 31.74190 seconds\n",
      "CPU Usage: 46.2 MHz\n",
      "Memory Used: 9597.21875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 15.01193\n",
      "MAE: 12.76935\n",
      "R-squared: 0.88147\n",
      "SMAPE: 0.40437\n",
      "Training Time: 79.68966 seconds\n",
      "CPU Usage: 62.1 MHz\n",
      "Memory Used: 9599.05078125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 13.29631\n",
      "MAE: 10.63343\n",
      "R-squared: 0.90701\n",
      "SMAPE: 0.39534\n",
      "Training Time: 69.27376 seconds\n",
      "CPU Usage: 64.4 MHz\n",
      "Memory Used: 9644.34765625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "LSTM (Units: 256, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 11.95047\n",
      "MAE: 9.61472\n",
      "R-squared: 0.92488\n",
      "SMAPE: 0.39099\n",
      "Training Time: 83.44995 seconds\n",
      "CPU Usage: 61.0 MHz\n",
      "Memory Used: 9643.37109375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "LSTM (Units: 256, Dropout Rate: 0.2, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 21.30991\n",
      "MAE: 16.38719\n",
      "R-squared: 0.76115\n",
      "SMAPE: 0.38948\n",
      "Training Time: 70.55599 seconds\n",
      "CPU Usage: 55.8 MHz\n",
      "Memory Used: 9667.375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 9ms/step\n",
      "True Values and Forecasts:\n",
      "    True Value  True Value  \\\n",
      "0         32.6        32.6   \n",
      "1         36.7        36.7   \n",
      "2         42.7        42.7   \n",
      "3         49.8        49.8   \n",
      "4         56.9        56.9   \n",
      "..         ...         ...   \n",
      "66       111.3       111.3   \n",
      "67       107.7       107.7   \n",
      "68       103.2       103.2   \n",
      "69        99.5        99.5   \n",
      "70        96.1        96.1   \n",
      "\n",
      "    LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.005, Batch Size: 8)  \n",
      "0                                           31.752762                          \n",
      "1                                           35.070770                          \n",
      "2                                           39.169647                          \n",
      "3                                           45.108242                          \n",
      "4                                           52.025650                          \n",
      "..                                                ...                          \n",
      "66                                         115.326302                          \n",
      "67                                         113.887993                          \n",
      "68                                         109.905884                          \n",
      "69                                         104.957275                          \n",
      "70                                         100.936661                          \n",
      "\n",
      "[71 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for LSTM model\n",
    "hyperparameters = {\n",
    "    'LSTM': {\n",
    "        'units': [8, 32, 64, 128, 256],\n",
    "        'dropout_rate': [0.0, 0.2],\n",
    "        'learning_rate': [0.001, 0.005],\n",
    "        'batch_size': [8]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate the LSTM model with hyperparameter tuning\n",
    "best_rmse = np.inf\n",
    "best_model = None\n",
    "best_units = 0\n",
    "best_dropout_rate = 0\n",
    "best_learning_rate = 0\n",
    "best_batch_size = 0\n",
    "\n",
    "for units in hyperparameters['LSTM']['units']:\n",
    "    for dropout_rate in hyperparameters['LSTM']['dropout_rate']:\n",
    "        for learning_rate in hyperparameters['LSTM']['learning_rate']:\n",
    "            for batch_size in hyperparameters['LSTM']['batch_size']:\n",
    "                model = create_lstm_model(units, dropout_rate)\n",
    "                model.optimizer.lr = learning_rate\n",
    "\n",
    "                start_time = time.time()\n",
    "                history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                    y_train,\n",
    "                                    epochs=100,\n",
    "                                    batch_size=batch_size,\n",
    "                                    verbose=0)\n",
    "                training_time = time.time() - start_time\n",
    "\n",
    "                predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                mse = mean_squared_error(y_test, predictions)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(y_test, predictions)\n",
    "                r2 = r2_score(y_test, predictions)\n",
    "                smape_val = smape(y_test, predictions)\n",
    "\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = model\n",
    "                    best_units = units\n",
    "                    best_dropout_rate = dropout_rate\n",
    "                    best_learning_rate = learning_rate\n",
    "                    best_batch_size = batch_size\n",
    "\n",
    "                print(f\"LSTM (Units: {units}, Dropout Rate: {dropout_rate}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                print(f\"RMSE: {rmse:.5f}\")\n",
    "                print(f\"MAE: {mae:.5f}\")\n",
    "                print(f\"R-squared: {r2:.5f}\")\n",
    "                print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                print()\n",
    "\n",
    "# Store the predictions in the dictionary\n",
    "model_data[f\"LSTM (Units: {best_units}, Dropout Rate: {best_dropout_rate}, Learning Rate: {best_learning_rate}, Batch Size: {best_batch_size})\"] = best_model.predict(\n",
    "    X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "# Include evaluation metrics and best hyperparameters in the eval metrics dictionary\n",
    "eval_metrics['LSTM'] = {\n",
    "    'Best Units': best_units,\n",
    "    'Best Dropout Rate': best_dropout_rate,\n",
    "    'Best Learning Rate': best_learning_rate,\n",
    "    'Best Batch Size': best_batch_size,\n",
    "    'RMSE': best_rmse,\n",
    "    'MAE': mae,\n",
    "    'R-squared': r2,\n",
    "    'SMAPE': smape_val,\n",
    "    'Training Time (seconds)': training_time,\n",
    "    'CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "    'Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024\n",
    "}\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('lstm_forecasts_1875_to_1941_2.csv', index=False)\n",
    "\n",
    "# Save evaluation metrics to a CSV file\n",
    "df_metrics = pd.DataFrame(eval_metrics).T\n",
    "df_metrics.to_csv('lstm_eval_metrics_1875_to_1941_2.csv')\n",
    "\n",
    "# Print true values and forecasts in table format\n",
    "df_true_values = pd.DataFrame(y_test, columns=['True Value'])\n",
    "df_forecasts = pd.DataFrame(model_data)\n",
    "\n",
    "print(\"True Values and Forecasts:\")\n",
    "print(pd.concat([df_true_values, df_forecasts], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41667ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression models 792 MONTHS (720 Months training, and 72 Testing) \n",
    "# Interval (1875-1941)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcce3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LinearRegression: {}\n",
      "LinearRegression RMSE: 4.90207\n",
      "LinearRegression MAE: 4.07230\n",
      "LinearRegression R-squared: 0.98736\n",
      "LinearRegression SMAPE: 0.40721\n",
      "LinearRegression Training Time: 0.18539 seconds\n",
      "CPU Usage: 23.8 MHz\n",
      "Memory Used: 12790.30859375 MB\n",
      "\n",
      "Best parameters for SVR: {'C': 10, 'epsilon': 0.001}\n",
      "SVR RMSE: 23.42446\n",
      "SVR MAE: 15.85800\n",
      "SVR R-squared: 0.71139\n",
      "SVR SMAPE: 0.39108\n",
      "SVR Training Time: 0.89304 seconds\n",
      "CPU Usage: 67.8 MHz\n",
      "Memory Used: 12783.12109375 MB\n",
      "\n",
      "Best parameters for AdaBoostRegressor: {'learning_rate': 0.1, 'n_estimators': 200}\n",
      "AdaBoostRegressor RMSE: 11.26306\n",
      "AdaBoostRegressor MAE: 8.48377\n",
      "AdaBoostRegressor R-squared: 0.93328\n",
      "AdaBoostRegressor SMAPE: 0.39472\n",
      "AdaBoostRegressor Training Time: 8.03174 seconds\n",
      "CPU Usage: 55.2 MHz\n",
      "Memory Used: 12713.02734375 MB\n",
      "\n",
      "Best parameters for RandomForestRegressor: {'max_depth': 5, 'n_estimators': 200}\n",
      "RandomForestRegressor RMSE: 8.74848\n",
      "RandomForestRegressor MAE: 6.67954\n",
      "RandomForestRegressor R-squared: 0.95974\n",
      "RandomForestRegressor SMAPE: 0.39759\n",
      "RandomForestRegressor Training Time: 8.27894 seconds\n",
      "CPU Usage: 40.1 MHz\n",
      "Memory Used: 12734.09375 MB\n",
      "\n",
      "Best parameters for GradientBoostingRegressor: {'learning_rate': 0.1, 'n_estimators': 50}\n",
      "GradientBoostingRegressor RMSE: 8.89410\n",
      "GradientBoostingRegressor MAE: 6.83024\n",
      "GradientBoostingRegressor R-squared: 0.95839\n",
      "GradientBoostingRegressor SMAPE: 0.39764\n",
      "GradientBoostingRegressor Training Time: 2.42986 seconds\n",
      "CPU Usage: 39.1 MHz\n",
      "Memory Used: 12752.34375 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the target variable\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# target_scaled = scaler.fit_transform(target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "if X_test.shape[0] == 0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Define the regression models to be evaluated\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    SVR(),\n",
    "    AdaBoostRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor()\n",
    "]\n",
    "\n",
    "# Hyperparameter grids for the regression models\n",
    "hyperparameter_grids = [\n",
    "    {},\n",
    "    {'C': [0.1, 1, 10], 'epsilon': [0.1, 0.01, 0.001]},\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]},\n",
    "    {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10]},\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "]\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Train and evaluate each model with hyperparameter tuning\n",
    "best_model_predictions = []\n",
    "best_model_smapes = []\n",
    "best_model_mses = []\n",
    "best_model_rmses = []\n",
    "best_model_maes = []\n",
    "best_model_r2s = []\n",
    "training_times = []\n",
    "cpu_usages = []\n",
    "memory_usages = []\n",
    "\n",
    "for model, hyperparameters in zip(models, hyperparameter_grids):\n",
    "    # Create a grid search instance\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=hyperparameters, scoring='neg_mean_squared_error', cv=3, n_jobs=1)\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train.ravel())\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Get the best model and its predictions\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_predictions = best_model.predict(X_test).ravel()\n",
    "\n",
    "    # Append the best predictions to the list\n",
    "    best_model_predictions.append(best_predictions)\n",
    "\n",
    "    # Compute evaluation metrics for the best model\n",
    "    mse = mean_squared_error(y_test, best_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, best_predictions)\n",
    "    r2 = r2_score(y_test, best_predictions)\n",
    "    smape_val = smape(y_test, best_predictions)\n",
    "\n",
    "    best_model_smapes.append(smape_val)\n",
    "    best_model_mses.append(mse)\n",
    "    best_model_rmses.append(rmse)\n",
    "    best_model_maes.append(mae)\n",
    "    best_model_r2s.append(r2)\n",
    "\n",
    "    # Print the best parameters and evaluation metrics\n",
    "    model_name = model.__class__.__name__\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} RMSE: {rmse:.5f}\")\n",
    "    print(f\"{model_name} MAE: {mae:.5f}\")\n",
    "    print(f\"{model_name} R-squared: {r2:.5f}\")\n",
    "    print(f\"{model_name} SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"{model_name} Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {cpu_usage:.1f} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "    # Append training time, CPU usage, and memory usage to lists\n",
    "    training_times.append(training_time)\n",
    "    cpu_usages.append(cpu_usage)\n",
    "    memory_usages.append(psutil.virtual_memory().used / 1024 / 1024)\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(best_model_predictions, axis=0)\n",
    "median_predictions = np.median(best_model_predictions, axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute harmonic mean and geometric mean of the predictions\n",
    "harmonic_mean_predictions = hmean(best_model_predictions)\n",
    "geometric_mean_predictions = gmean(best_model_predictions)\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': [model.__class__.__name__ for model in models] + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': best_model_rmses + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': best_model_maes + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': best_model_r2s + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': best_model_smapes + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': training_times + [None] * 4,\n",
    "    'CPU Usage (MHz)': cpu_usages + [None] * 4,\n",
    "    'Memory Used (MB)': memory_usages + [None] * 4\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('Reg_eval_metrics_792_Months_1875_to_1941.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33184a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LinearRegression: {}\n",
      "LinearRegression RMSE: 3.62732\n",
      "LinearRegression MAE: 2.85071\n",
      "LinearRegression R-squared: 0.98806\n",
      "LinearRegression SMAPE: 0.48255\n",
      "LinearRegression Training Time: 0.03923 seconds\n",
      "CPU Usage: 9.2 MHz\n",
      "Memory Used: 8984.625 MB\n",
      "\n",
      "Best parameters for SVR: {'C': 10, 'epsilon': 0.1}\n",
      "SVR RMSE: 3.76407\n",
      "SVR MAE: 2.94968\n",
      "SVR R-squared: 0.98714\n",
      "SVR SMAPE: 0.48638\n",
      "SVR Training Time: 1.45788 seconds\n",
      "CPU Usage: 37.3 MHz\n",
      "Memory Used: 8992.79296875 MB\n",
      "\n",
      "Best parameters for AdaBoostRegressor: {'learning_rate': 0.1, 'n_estimators': 200}\n",
      "AdaBoostRegressor RMSE: 4.58039\n",
      "AdaBoostRegressor MAE: 3.77324\n",
      "AdaBoostRegressor R-squared: 0.98096\n",
      "AdaBoostRegressor SMAPE: 0.49504\n",
      "AdaBoostRegressor Training Time: 7.44464 seconds\n",
      "CPU Usage: 23.5 MHz\n",
      "Memory Used: 8989.75 MB\n",
      "\n",
      "Best parameters for RandomForestRegressor: {'max_depth': 5, 'n_estimators': 50}\n",
      "RandomForestRegressor RMSE: 3.98137\n",
      "RandomForestRegressor MAE: 3.21772\n",
      "RandomForestRegressor R-squared: 0.98561\n",
      "RandomForestRegressor SMAPE: 0.48423\n",
      "RandomForestRegressor Training Time: 8.68184 seconds\n",
      "CPU Usage: 22.1 MHz\n",
      "Memory Used: 8988.40625 MB\n",
      "\n",
      "Best parameters for GradientBoostingRegressor: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "GradientBoostingRegressor RMSE: 3.89473\n",
      "GradientBoostingRegressor MAE: 3.09801\n",
      "GradientBoostingRegressor R-squared: 0.98623\n",
      "GradientBoostingRegressor SMAPE: 0.48431\n",
      "GradientBoostingRegressor Training Time: 3.91356 seconds\n",
      "CPU Usage: 19.8 MHz\n",
      "Memory Used: 8980.85546875 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1854 to 1931)\n",
    "filtered_data = data[(data['Year'] >= 1854) & (data['Year'] <= 1931)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the target variable\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# target_scaled = scaler.fit_transform(target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 840  # Number of months for training\n",
    "test_size = 84  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "if X_test.shape[0] == 0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Define the regression models to be evaluated\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    SVR(),\n",
    "    AdaBoostRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor()\n",
    "]\n",
    "\n",
    "# Hyperparameter grids for the regression models\n",
    "hyperparameter_grids = [\n",
    "    {},\n",
    "    {'C': [0.1, 1, 10], 'epsilon': [0.1, 0.01, 0.001]},\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]},\n",
    "    {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10]},\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "]\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Train and evaluate each model with hyperparameter tuning\n",
    "best_model_predictions = []\n",
    "best_model_smapes = []\n",
    "best_model_mses = []\n",
    "best_model_rmses = []\n",
    "best_model_maes = []\n",
    "best_model_r2s = []\n",
    "training_times = []\n",
    "cpu_usages = []\n",
    "memory_usages = []\n",
    "\n",
    "for model, hyperparameters in zip(models, hyperparameter_grids):\n",
    "    # Create a grid search instance\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=hyperparameters, scoring='neg_mean_squared_error', cv=3, n_jobs=1)\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train.ravel())\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Get the best model and its predictions\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_predictions = best_model.predict(X_test).ravel()\n",
    "\n",
    "    # Append the best predictions to the list\n",
    "    best_model_predictions.append(best_predictions)\n",
    "\n",
    "    # Compute evaluation metrics for the best model\n",
    "    mse = mean_squared_error(y_test, best_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, best_predictions)\n",
    "    r2 = r2_score(y_test, best_predictions)\n",
    "    smape_val = smape(y_test, best_predictions)\n",
    "\n",
    "    best_model_smapes.append(smape_val)\n",
    "    best_model_mses.append(mse)\n",
    "    best_model_rmses.append(rmse)\n",
    "    best_model_maes.append(mae)\n",
    "    best_model_r2s.append(r2)\n",
    "\n",
    "    # Print the best parameters and evaluation metrics\n",
    "    model_name = model.__class__.__name__\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} RMSE: {rmse:.5f}\")\n",
    "    print(f\"{model_name} MAE: {mae:.5f}\")\n",
    "    print(f\"{model_name} R-squared: {r2:.5f}\")\n",
    "    print(f\"{model_name} SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"{model_name} Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {cpu_usage:.1f} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "    # Append training time, CPU usage, and memory usage to lists\n",
    "    training_times.append(training_time)\n",
    "    cpu_usages.append(cpu_usage)\n",
    "    memory_usages.append(psutil.virtual_memory().used / 1024 / 1024)\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(best_model_predictions, axis=0)\n",
    "median_predictions = np.median(best_model_predictions, axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute harmonic mean and geometric mean of the predictions\n",
    "harmonic_mean_predictions = hmean(best_model_predictions)\n",
    "geometric_mean_predictions = gmean(best_model_predictions)\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': [model.__class__.__name__ for model in models] + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': best_model_rmses + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': best_model_maes + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': best_model_r2s + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': best_model_smapes + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': training_times + [None] * 4,\n",
    "    'CPU Usage (MHz)': cpu_usages + [None] * 4,\n",
    "    'Memory Used (MB)': memory_usages + [None] * 4\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('Reg_eval_metrics_924_Months_1854_to_1931.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d764f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LinearRegression: {}\n",
      "LinearRegression RMSE: 3.26000\n",
      "LinearRegression MAE: 2.71881\n",
      "LinearRegression R-squared: 0.99690\n",
      "LinearRegression SMAPE: 0.83592\n",
      "LinearRegression Training Time: 0.03021 seconds\n",
      "CPU Usage: 11.2 MHz\n",
      "Memory Used: 9406.24609375 MB\n",
      "\n",
      "Best parameters for SVR: {'C': 10, 'epsilon': 0.1}\n",
      "SVR RMSE: 3.00136\n",
      "SVR MAE: 2.45326\n",
      "SVR R-squared: 0.99738\n",
      "SVR SMAPE: 0.83531\n",
      "SVR Training Time: 1.45736 seconds\n",
      "CPU Usage: 28.0 MHz\n",
      "Memory Used: 9417.50390625 MB\n",
      "\n",
      "Best parameters for AdaBoostRegressor: {'learning_rate': 0.1, 'n_estimators': 200}\n",
      "AdaBoostRegressor RMSE: 4.94923\n",
      "AdaBoostRegressor MAE: 4.18209\n",
      "AdaBoostRegressor R-squared: 0.99287\n",
      "AdaBoostRegressor SMAPE: 0.82498\n",
      "AdaBoostRegressor Training Time: 7.50766 seconds\n",
      "CPU Usage: 21.0 MHz\n",
      "Memory Used: 9417.0625 MB\n",
      "\n",
      "Best parameters for RandomForestRegressor: {'max_depth': 10, 'n_estimators': 50}\n",
      "RandomForestRegressor RMSE: 4.30944\n",
      "RandomForestRegressor MAE: 3.10853\n",
      "RandomForestRegressor R-squared: 0.99459\n",
      "RandomForestRegressor SMAPE: 0.84120\n",
      "RandomForestRegressor Training Time: 9.02998 seconds\n",
      "CPU Usage: 19.3 MHz\n",
      "Memory Used: 9416.2890625 MB\n",
      "\n",
      "Best parameters for GradientBoostingRegressor: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "GradientBoostingRegressor RMSE: 3.54681\n",
      "GradientBoostingRegressor MAE: 2.81387\n",
      "GradientBoostingRegressor R-squared: 0.99634\n",
      "GradientBoostingRegressor SMAPE: 0.83874\n",
      "GradientBoostingRegressor Training Time: 4.08098 seconds\n",
      "CPU Usage: 18.2 MHz\n",
      "Memory Used: 9417.23046875 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import hmean, gmean\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1931 to 2008)\n",
    "filtered_data = data[(data['Year'] >= 1931) & (data['Year'] <= 2008)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the target variable\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# target_scaled = scaler.fit_transform(target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 840  # Number of months for training\n",
    "test_size = 84  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "if X_test.shape[0] == 0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Define the regression models to be evaluated\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    SVR(),\n",
    "    AdaBoostRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor()\n",
    "]\n",
    "\n",
    "# Hyperparameter grids for the regression models\n",
    "hyperparameter_grids = [\n",
    "    {},\n",
    "    {'C': [0.1, 1, 10], 'epsilon': [0.1, 0.01, 0.001]},\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]},\n",
    "    {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10]},\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "]\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Train and evaluate each model with hyperparameter tuning\n",
    "best_model_predictions = []\n",
    "best_model_smapes = []\n",
    "best_model_mses = []\n",
    "best_model_rmses = []\n",
    "best_model_maes = []\n",
    "best_model_r2s = []\n",
    "training_times = []\n",
    "cpu_usages = []\n",
    "memory_usages = []\n",
    "\n",
    "for model, hyperparameters in zip(models, hyperparameter_grids):\n",
    "    # Create a grid search instance\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=hyperparameters, scoring='neg_mean_squared_error', cv=3, n_jobs=1)\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train.ravel())\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Get the best model and its predictions\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_predictions = best_model.predict(X_test).ravel()\n",
    "\n",
    "    # Append the best predictions to the list\n",
    "    best_model_predictions.append(best_predictions)\n",
    "\n",
    "    # Compute evaluation metrics for the best model\n",
    "    mse = mean_squared_error(y_test, best_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, best_predictions)\n",
    "    r2 = r2_score(y_test, best_predictions)\n",
    "    smape_val = smape(y_test, best_predictions)\n",
    "\n",
    "    best_model_smapes.append(smape_val)\n",
    "    best_model_mses.append(mse)\n",
    "    best_model_rmses.append(rmse)\n",
    "    best_model_maes.append(mae)\n",
    "    best_model_r2s.append(r2)\n",
    "\n",
    "    # Print the best parameters and evaluation metrics\n",
    "    model_name = model.__class__.__name__\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} RMSE: {rmse:.5f}\")\n",
    "    print(f\"{model_name} MAE: {mae:.5f}\")\n",
    "    print(f\"{model_name} R-squared: {r2:.5f}\")\n",
    "    print(f\"{model_name} SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"{model_name} Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {cpu_usage:.1f} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "    # Append training time, CPU usage, and memory usage to lists\n",
    "    training_times.append(training_time)\n",
    "    cpu_usages.append(cpu_usage)\n",
    "    memory_usages.append(psutil.virtual_memory().used / 1024 / 1024)\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(best_model_predictions, axis=0)\n",
    "median_predictions = np.median(best_model_predictions, axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute harmonic mean and geometric mean of the predictions\n",
    "harmonic_mean_predictions = hmean(best_model_predictions)\n",
    "geometric_mean_predictions = gmean(best_model_predictions)\n",
    "\n",
    "# Compute evaluation metrics for harmonic mean predictions\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for geometric mean predictions\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': [model.__class__.__name__ for model in models] + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': best_model_rmses + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': best_model_maes + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': best_model_r2s + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': best_model_smapes + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': training_times + [None] * 4,\n",
    "    'CPU Usage (MHz)': cpu_usages + [None] * 4,\n",
    "    'Memory Used (MB)': memory_usages + [None] * 4\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('Reg_eval_metrics_924_Months_1931_to_2008.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0271b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store evaluation metrics\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "r2_list = []\n",
    "smape_list = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                        y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=8,\n",
    "                        verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    smape_val = smape(y_test, predictions)\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = predictions\n",
    "\n",
    "    # Append evaluation metrics to the lists\n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "    smape_list.append(smape_val)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"RMSE: {rmse:.5f}\")\n",
    "    print(f\"MAE: {mae:.5f}\")\n",
    "    print(f\"R-squared: {r2:.5f}\")\n",
    "    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "    print()\n",
    "\n",
    "# Combine the predictions of the models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Append the combined and median predictions to the model data dictionary\n",
    "model_data['Combined Model'] = combined_predictions\n",
    "model_data['Median Model'] = median_predictions\n",
    "\n",
    "# Append the harmonic mean and geometric mean predictions to the model data dictionary\n",
    "model_data['Harmonic Mean Model'] = harmonic_mean_predictions\n",
    "model_data['Geometric Mean Model'] = geometric_mean_predictions\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL_forecasts_test3.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(models.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': rmse_list + [np.sqrt(mean_squared_error(y_test, combined_predictions)),\n",
    "                         np.sqrt(mean_squared_error(y_test, median_predictions)),\n",
    "                         np.sqrt(mean_squared_error(y_test, harmonic_mean_predictions)),\n",
    "                         np.sqrt(mean_squared_error(y_test, geometric_mean_predictions))],\n",
    "    'MAE': mae_list + [mean_absolute_error(y_test, combined_predictions),\n",
    "                       mean_absolute_error(y_test, median_predictions),\n",
    "                       mean_absolute_error(y_test, harmonic_mean_predictions),\n",
    "                       mean_absolute_error(y_test, geometric_mean_predictions)],\n",
    "    'R-squared': r2_list + [r2_score(y_test, combined_predictions),\n",
    "                           r2_score(y_test, median_predictions),\n",
    "                           r2_score(y_test, harmonic_mean_predictions),\n",
    "                           r2_score(y_test, geometric_mean_predictions)],\n",
    "    'SMAPE': smape_list + [smape(y_test, combined_predictions),\n",
    "                           smape(y_test, median_predictions),\n",
    "                           smape(y_test, harmonic_mean_predictions),\n",
    "                           smape(y_test, geometric_mean_predictions)],\n",
    "    'Training Time (seconds)': [training_time] * len(models) + [None, None, None, None],\n",
    "    'CPU Usage (MHz)': [psutil.cpu_percent()] * len(models) + [None, None, None, None],\n",
    "    'Memory Used (MB)': [psutil.virtual_memory().used / 1024 / 1024] * len(models) + [None, None, None, None],\n",
    "    'Best Parameters': [None] * len(models) + [None, None, None, None]\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL_metrics_test3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'fit_intercept': [True, False],  # Whether to include an intercept term\n",
    "    'normalize': [True, False],  # Whether to normalize the input features\n",
    "    'copy_X': [True, False],  # Whether to copy the input features or modify them in-place\n",
    "    'n_jobs': [None, -1],  # Number of parallel jobs to run (-1 indicates using all available processors)\n",
    "    'positive': [True, False],  # Whether to enforce positive coefficients\n",
    "    'selection': ['cyclic', 'random'],  # Method for coefficient selection in Lasso (L1 regularization)\n",
    "    'tol': [1e-4, 1e-5, 1e-6],  # Tolerance for stopping criteria\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],  # Solver algorithm for optimization\n",
    "    'alpha_lasso': [0.1, 1.0, 10.0],  # Regularization strength (penalty term) for Lasso (L1) regularization\n",
    "    'alpha_ridge': [0.1, 1.0, 10.0],  # Regularization strength (penalty term) for Ridge (L2) regularization\n",
    "    'l1_ratio': [0.1, 0.5, 0.9]  # L1 ratio for Elastic Net regularization\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730177e3",
   "metadata": {},
   "source": [
    "# Deep learning 792 MONTHS (720 Months training, and 72 Testing) \n",
    "# Interval (1875-1941)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12871f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 128, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 18.70207\n",
      "MAE: 14.82553\n",
      "R-squared: 0.81603\n",
      "SMAPE: 0.39547\n",
      "Training Time: 19.95766 seconds\n",
      "CPU Usage: 18.5 MHz\n",
      "Memory Used: 13691.76953125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: LSTM (Units: 15, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 66.45579\n",
      "MAE: 56.53208\n",
      "R-squared: -1.32293\n",
      "SMAPE: 0.55470\n",
      "Training Time: 18.88808 seconds\n",
      "CPU Usage: 24.6 MHz\n",
      "Memory Used: 13736.640625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 128, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 17.07815\n",
      "MAE: 13.25761\n",
      "R-squared: 0.84659\n",
      "SMAPE: 0.39511\n",
      "Training Time: 48.08800 seconds\n",
      "CPU Usage: 36.5 MHz\n",
      "Memory Used: 13744.66796875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Stacked LSTM (Units: 15, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 65.44545\n",
      "MAE: 55.13991\n",
      "R-squared: -1.25283\n",
      "SMAPE: 0.54238\n",
      "Training Time: 29.88610 seconds\n",
      "CPU Usage: 28.7 MHz\n",
      "Memory Used: 13784.328125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 15.36035\n",
      "MAE: 12.25809\n",
      "R-squared: 0.87590\n",
      "SMAPE: 0.39332\n",
      "Training Time: 34.96620 seconds\n",
      "CPU Usage: 38.0 MHz\n",
      "Memory Used: 13798.95703125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 15, Learning Rate: 0.005, Batch Size: 8)\n",
      "RMSE: 40.84287\n",
      "MAE: 31.63232\n",
      "R-squared: 0.12259\n",
      "SMAPE: 0.41036\n",
      "Training Time: 25.37994 seconds\n",
      "CPU Usage: 36.2 MHz\n",
      "Memory Used: 13796.46875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 128, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 18.50950\n",
      "MAE: 15.07948\n",
      "R-squared: 0.81980\n",
      "SMAPE: 0.39956\n",
      "Training Time: 26.52655 seconds\n",
      "CPU Usage: 25.2 MHz\n",
      "Memory Used: 13809.5390625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 15, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 57.53588\n",
      "MAE: 47.35533\n",
      "R-squared: -0.74119\n",
      "SMAPE: 0.48643\n",
      "Training Time: 21.60990 seconds\n",
      "CPU Usage: 31.3 MHz\n",
      "Memory Used: 13864.7421875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean, gmean\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [128, 15], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Stacked LSTM': {'units': [128, 15], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Bidirectional LSTM': {'units': [128, 15], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'GRU': {'units': [128, 15], 'learning_rate': [0.001], 'batch_size': [8]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for units in params['units']:\n",
    "        for learning_rate in params['learning_rate']:\n",
    "            for batch_size in params['batch_size']:\n",
    "                if model_name == 'LSTM':\n",
    "                    model = create_lstm_model(units, learning_rate, batch_size)\n",
    "                elif model_name == 'Stacked LSTM':\n",
    "                    model = create_stacked_lstm_model(units, learning_rate, batch_size)\n",
    "                elif model_name == 'Bidirectional LSTM':\n",
    "                    model = create_bidirectional_lstm_model(units, learning_rate, batch_size)\n",
    "                elif model_name == 'GRU':\n",
    "                    model = create_gru_model(units, learning_rate, batch_size)\n",
    "\n",
    "                start_time = time.time()\n",
    "                history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                    y_train,\n",
    "                                    epochs=100,\n",
    "                                    batch_size=batch_size,\n",
    "                                    verbose=0)\n",
    "                training_time = time.time() - start_time\n",
    "\n",
    "                predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                mse = mean_squared_error(y_test, predictions)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(y_test, predictions)\n",
    "                r2 = r2_score(y_test, predictions)\n",
    "                smape_val = smape(y_test, predictions)\n",
    "\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = model\n",
    "\n",
    "                print(f\"Model: {model_name} (Units: {units}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                print(f\"RMSE: {rmse:.5f}\")\n",
    "                print(f\"MAE: {mae:.5f}\")\n",
    "                print(f\"R-squared: {r2:.5f}\")\n",
    "                print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = best_model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'RMSE': best_rmse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2,\n",
    "        'SMAPE': smape_val,\n",
    "        'Training Time (seconds)': training_time,\n",
    "        'CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024\n",
    "    }\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the harmonic mean model\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the geometric mean model\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL_forecasts_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(hyperparameters.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': [eval_metrics[model]['RMSE'] for model in hyperparameters.keys()] + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': [eval_metrics[model]['MAE'] for model in hyperparameters.keys()] + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': [eval_metrics[model]['R-squared'] for model in hyperparameters.keys()] + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': [eval_metrics[model]['SMAPE'] for model in hyperparameters.keys()] + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': [eval_metrics[model]['Training Time (seconds)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'CPU Usage (MHz)': [eval_metrics[model]['CPU Usage (MHz)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Memory Used (MB)': [eval_metrics[model]['Memory Used (MB)'] for model in hyperparameters.keys()] + [None, None, None, None]\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL_eval_metrics_792_Months_1875_to_1941.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb9f959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 1ms/step\n",
      "Model: LSTM (Units: 32, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 38.03047\n",
      "MAE: 29.01166\n",
      "R-squared: 0.23927\n",
      "SMAPE: 0.39805\n",
      "Training Time: 17.20945 seconds\n",
      "CPU Usage: 19.3 MHz\n",
      "Memory Used: 15803.76171875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 64, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 24.43683\n",
      "MAE: 18.72685\n",
      "R-squared: 0.68591\n",
      "SMAPE: 0.39224\n",
      "Training Time: 18.74232 seconds\n",
      "CPU Usage: 23.6 MHz\n",
      "Memory Used: 15834.109375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 128, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 16.22849\n",
      "MAE: 12.46412\n",
      "R-squared: 0.86148\n",
      "SMAPE: 0.38390\n",
      "Training Time: 23.61059 seconds\n",
      "CPU Usage: 29.8 MHz\n",
      "Memory Used: 15873.1875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Stacked LSTM (Units: 32, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 38.60747\n",
      "MAE: 29.82029\n",
      "R-squared: 0.21601\n",
      "SMAPE: 0.40810\n",
      "Training Time: 26.78897 seconds\n",
      "CPU Usage: 29.1 MHz\n",
      "Memory Used: 15871.08203125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Stacked LSTM (Units: 64, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 25.26337\n",
      "MAE: 19.04838\n",
      "R-squared: 0.66430\n",
      "SMAPE: 0.39050\n",
      "Training Time: 35.07744 seconds\n",
      "CPU Usage: 34.6 MHz\n",
      "Memory Used: 15982.859375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 128, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 15.80168\n",
      "MAE: 12.01041\n",
      "R-squared: 0.86867\n",
      "SMAPE: 0.38042\n",
      "Training Time: 45.06835 seconds\n",
      "CPU Usage: 39.8 MHz\n",
      "Memory Used: 16082.2890625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 32, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 24.02409\n",
      "MAE: 18.26889\n",
      "R-squared: 0.69643\n",
      "SMAPE: 0.38980\n",
      "Training Time: 24.64263 seconds\n",
      "CPU Usage: 38.0 MHz\n",
      "Memory Used: 16076.0859375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 64, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 18.16536\n",
      "MAE: 14.05733\n",
      "R-squared: 0.82644\n",
      "SMAPE: 0.38935\n",
      "Training Time: 32.05688 seconds\n",
      "CPU Usage: 42.5 MHz\n",
      "Memory Used: 16094.7265625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 12.73134\n",
      "MAE: 10.11405\n",
      "R-squared: 0.91475\n",
      "SMAPE: 0.38890\n",
      "Training Time: 41.47597 seconds\n",
      "CPU Usage: 40.9 MHz\n",
      "Memory Used: 16099.0859375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 32, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 31.77890\n",
      "MAE: 24.29150\n",
      "R-squared: 0.46881\n",
      "SMAPE: 0.39745\n",
      "Training Time: 17.39181 seconds\n",
      "CPU Usage: 26.0 MHz\n",
      "Memory Used: 16096.06640625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 64, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 19.85243\n",
      "MAE: 15.03489\n",
      "R-squared: 0.79270\n",
      "SMAPE: 0.38371\n",
      "Training Time: 18.14946 seconds\n",
      "CPU Usage: 26.4 MHz\n",
      "Memory Used: 16135.30078125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Model: GRU (Units: 128, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 14.52040\n",
      "MAE: 11.19709\n",
      "R-squared: 0.88910\n",
      "SMAPE: 0.38766\n",
      "Training Time: 22.37865 seconds\n",
      "CPU Usage: 27.2 MHz\n",
      "Memory Used: 16144.8046875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean, gmean\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [32, 64, 128], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Stacked LSTM': {'units': [32, 64, 128], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Bidirectional LSTM': {'units': [32, 64, 128], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'GRU': {'units': [32, 64, 128], 'learning_rate': [0.001], 'batch_size': [8]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    for units in params['units']:\n",
    "        for learning_rate in params['learning_rate']:\n",
    "            for batch_size in params['batch_size']:\n",
    "                if model_name == 'LSTM':\n",
    "                    model = create_lstm_model(units, learning_rate, batch_size)\n",
    "                elif model_name == 'Stacked LSTM':\n",
    "                    model = create_stacked_lstm_model(units, learning_rate, batch_size)\n",
    "                elif model_name == 'Bidirectional LSTM':\n",
    "                    model = create_bidirectional_lstm_model(units, learning_rate, batch_size)\n",
    "                elif model_name == 'GRU':\n",
    "                    model = create_gru_model(units, learning_rate, batch_size)\n",
    "\n",
    "                start_time = time.time()\n",
    "                history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                    y_train,\n",
    "                                    epochs=100,\n",
    "                                    batch_size=batch_size,\n",
    "                                    verbose=0)\n",
    "                training_time = time.time() - start_time\n",
    "\n",
    "                predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                mse = mean_squared_error(y_test, predictions)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(y_test, predictions)\n",
    "                r2 = r2_score(y_test, predictions)\n",
    "                smape_val = smape(y_test, predictions)\n",
    "\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = model\n",
    "                    best_params = {\n",
    "                        'Units': units,\n",
    "                        'Learning Rate': learning_rate,\n",
    "                        'Batch Size': batch_size\n",
    "                    }\n",
    "\n",
    "                print(f\"Model: {model_name} (Units: {units}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                print(f\"RMSE: {rmse:.5f}\")\n",
    "                print(f\"MAE: {mae:.5f}\")\n",
    "                print(f\"R-squared: {r2:.5f}\")\n",
    "                print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = best_model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics and best parameters in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'Best RMSE': best_rmse,\n",
    "        'Best MAE': mae,\n",
    "        'Best R-squared': r2,\n",
    "        'Best SMAPE': smape_val,\n",
    "        'Best Training Time (seconds)': training_time,\n",
    "        'Best CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Best Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the harmonic mean model\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the geometric mean model\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL_forecasts_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(hyperparameters.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': [eval_metrics[model]['Best RMSE'] for model in hyperparameters.keys()] + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': [eval_metrics[model]['Best MAE'] for model in hyperparameters.keys()] + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': [eval_metrics[model]['Best R-squared'] for model in hyperparameters.keys()] + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': [eval_metrics[model]['Best SMAPE'] for model in hyperparameters.keys()] + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': [eval_metrics[model]['Best Training Time (seconds)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'CPU Usage (MHz)': [eval_metrics[model]['Best CPU Usage (MHz)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Memory Used (MB)': [eval_metrics[model]['Best Memory Used (MB)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Best Parameters': [eval_metrics[model]['Best Parameters'] for model in hyperparameters.keys()] + [None, None, None, None]\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL5_eval_metrics_792_Months_1875_to_1941.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1613c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean, gmean\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [8, 32, 64, 128, 256], 'dropout_rate': [0.0, 0.2, 0.4, 0.6, 0.8], 'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3], 'batch_size': [8, 16, 32, 64, 128]},\n",
    "    'Stacked LSTM': {'units': [8, 32, 64, 128, 256], 'dropout_rate': [0.0, 0.2, 0.4, 0.6, 0.8], 'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3], 'batch_size': [8, 16, 32, 64, 128]},\n",
    "    'Bidirectional LSTM': {'units': [8, 32, 64, 128, 256], 'dropout_rate': [0.0, 0.2, 0.4, 0.6, 0.8], 'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3], 'batch_size': [8, 16, 32, 64, 128]},\n",
    "    'GRU': {'units': [8, 32, 64, 128, 256], 'dropout_rate': [0.0, 0.2, 0.4, 0.6, 0.8], 'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3], 'batch_size': [8, 16, 32, 64, 128]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    for units in params['units']:\n",
    "        for dropout_rate in params['dropout_rate']:\n",
    "            for learning_rate in params['learning_rate']:\n",
    "                for batch_size in params['batch_size']:\n",
    "                    if model_name == 'LSTM':\n",
    "                        model = create_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Stacked LSTM':\n",
    "                        model = create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Bidirectional LSTM':\n",
    "                        model = create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'GRU':\n",
    "                        model = create_gru_model(units, dropout_rate, learning_rate, batch_size)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                        y_train,\n",
    "                                        epochs=100,\n",
    "                                        batch_size=batch_size,\n",
    "                                        verbose=0)\n",
    "                    training_time = time.time() - start_time\n",
    "\n",
    "                    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                    mse = mean_squared_error(y_test, predictions)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_test, predictions)\n",
    "                    r2 = r2_score(y_test, predictions)\n",
    "                    smape_val = smape(y_test, predictions)\n",
    "\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = model\n",
    "                        best_params = {\n",
    "                            'Units': units,\n",
    "                            'Dropout Rate': dropout_rate,\n",
    "                            'Learning Rate': learning_rate,\n",
    "                            'Batch Size': batch_size\n",
    "                        }\n",
    "\n",
    "                    print(f\"Model: {model_name} (Units: {units}, Dropout Rate: {dropout_rate}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                    print(f\"RMSE: {rmse:.5f}\")\n",
    "                    print(f\"MAE: {mae:.5f}\")\n",
    "                    print(f\"R-squared: {r2:.5f}\")\n",
    "                    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                    print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = best_model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics and best parameters in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'Best RMSE': best_rmse,\n",
    "        'Best MAE': mae,\n",
    "        'Best R-squared': r2,\n",
    "        'Best SMAPE': smape_val,\n",
    "        'Best Training Time (seconds)': training_time,\n",
    "        'Best CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Best Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the harmonic mean model\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the geometric mean model\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL_forecasts_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(hyperparameters.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': [eval_metrics[model]['Best RMSE'] for model in hyperparameters.keys()] + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': [eval_metrics[model]['Best MAE'] for model in hyperparameters.keys()] + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': [eval_metrics[model]['Best R-squared'] for model in hyperparameters.keys()] + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': [eval_metrics[model]['Best SMAPE'] for model in hyperparameters.keys()] + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': [eval_metrics[model]['Best Training Time (seconds)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'CPU Usage (MHz)': [eval_metrics[model]['Best CPU Usage (MHz)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Memory Used (MB)': [eval_metrics[model]['Best Memory Used (MB)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Best Parameters': [eval_metrics[model]['Best Parameters'] for model in hyperparameters.keys()] + [None, None, None, None]\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL6_eval_metrics_792_Months_1875_to_1941.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ec5f5",
   "metadata": {},
   "source": [
    "# Reported DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55965743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 8, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 91.42544\n",
      "MAE: 81.81916\n",
      "R-squared: -3.39647\n",
      "SMAPE: 0.80417\n",
      "Training Time: 16.85487 seconds\n",
      "CPU Usage: 27.2 MHz\n",
      "Memory Used: 16181.10546875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 8, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 92.10337\n",
      "MAE: 82.50260\n",
      "R-squared: -3.46191\n",
      "SMAPE: 0.81196\n",
      "Training Time: 18.19880 seconds\n",
      "CPU Usage: 26.7 MHz\n",
      "Memory Used: 16176.203125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 32, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 38.26613\n",
      "MAE: 29.41134\n",
      "R-squared: 0.22981\n",
      "SMAPE: 0.40468\n",
      "Training Time: 17.45640 seconds\n",
      "CPU Usage: 24.1 MHz\n",
      "Memory Used: 16154.16796875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 32, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 38.95418\n",
      "MAE: 30.30908\n",
      "R-squared: 0.20186\n",
      "SMAPE: 0.40923\n",
      "Training Time: 17.28889 seconds\n",
      "CPU Usage: 25.2 MHz\n",
      "Memory Used: 16167.23046875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 64, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 21.80707\n",
      "MAE: 16.51134\n",
      "R-squared: 0.74987\n",
      "SMAPE: 0.39006\n",
      "Training Time: 18.32925 seconds\n",
      "CPU Usage: 25.3 MHz\n",
      "Memory Used: 16164.6484375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 64, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 23.73007\n",
      "MAE: 18.17108\n",
      "R-squared: 0.70381\n",
      "SMAPE: 0.39163\n",
      "Training Time: 19.06054 seconds\n",
      "CPU Usage: 26.0 MHz\n",
      "Memory Used: 16184.21484375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 12.31579\n",
      "MAE: 9.88050\n",
      "R-squared: 0.92022\n",
      "SMAPE: 0.38932\n",
      "Training Time: 22.39253 seconds\n",
      "CPU Usage: 25.7 MHz\n",
      "Memory Used: 16194.7890625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: LSTM (Units: 128, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 18.15012\n",
      "MAE: 14.37564\n",
      "R-squared: 0.82673\n",
      "SMAPE: 0.39635\n",
      "Training Time: 23.18726 seconds\n",
      "CPU Usage: 25.9 MHz\n",
      "Memory Used: 16194.01953125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 9.52824\n",
      "MAE: 7.74081\n",
      "R-squared: 0.95225\n",
      "SMAPE: 0.38168\n",
      "Training Time: 62.74021 seconds\n",
      "CPU Usage: 50.9 MHz\n",
      "Memory Used: 16190.03125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 256, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 12.77979\n",
      "MAE: 10.27133\n",
      "R-squared: 0.91410\n",
      "SMAPE: 0.39338\n",
      "Training Time: 72.04900 seconds\n",
      "CPU Usage: 52.8 MHz\n",
      "Memory Used: 16267.63671875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Stacked LSTM (Units: 8, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 89.03805\n",
      "MAE: 79.47842\n",
      "R-squared: -3.16985\n",
      "SMAPE: 0.77695\n",
      "Training Time: 24.14690 seconds\n",
      "CPU Usage: 25.6 MHz\n",
      "Memory Used: 16259.05859375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Stacked LSTM (Units: 8, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 91.08867\n",
      "MAE: 81.32569\n",
      "R-squared: -3.36414\n",
      "SMAPE: 0.79761\n",
      "Training Time: 26.56844 seconds\n",
      "CPU Usage: 27.9 MHz\n",
      "Memory Used: 16304.21484375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 32, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 34.95655\n",
      "MAE: 26.24328\n",
      "R-squared: 0.35727\n",
      "SMAPE: 0.38921\n",
      "Training Time: 24.65757 seconds\n",
      "CPU Usage: 27.6 MHz\n",
      "Memory Used: 16301.9453125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Stacked LSTM (Units: 32, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 38.80952\n",
      "MAE: 29.52439\n",
      "R-squared: 0.20778\n",
      "SMAPE: 0.40048\n",
      "Training Time: 26.73781 seconds\n",
      "CPU Usage: 26.1 MHz\n",
      "Memory Used: 16372.421875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Stacked LSTM (Units: 64, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 19.52675\n",
      "MAE: 14.50686\n",
      "R-squared: 0.79945\n",
      "SMAPE: 0.38476\n",
      "Training Time: 33.12868 seconds\n",
      "CPU Usage: 27.6 MHz\n",
      "Memory Used: 16381.5859375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Stacked LSTM (Units: 64, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 24.08483\n",
      "MAE: 17.92143\n",
      "R-squared: 0.69489\n",
      "SMAPE: 0.38151\n",
      "Training Time: 35.35505 seconds\n",
      "CPU Usage: 31.4 MHz\n",
      "Memory Used: 16441.5390625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 12.76704\n",
      "MAE: 9.62641\n",
      "R-squared: 0.91427\n",
      "SMAPE: 0.38795\n",
      "Training Time: 52.47095 seconds\n",
      "CPU Usage: 38.3 MHz\n",
      "Memory Used: 15854.89453125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Stacked LSTM (Units: 128, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 17.45913\n",
      "MAE: 13.02340\n",
      "R-squared: 0.83967\n",
      "SMAPE: 0.37379\n",
      "Training Time: 44.27985 seconds\n",
      "CPU Usage: 34.3 MHz\n",
      "Memory Used: 15877.625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Stacked LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 10.50363\n",
      "MAE: 8.33125\n",
      "R-squared: 0.94197\n",
      "SMAPE: 0.39416\n",
      "Training Time: 126.95315 seconds\n",
      "CPU Usage: 77.1 MHz\n",
      "Memory Used: 15892.9296875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Stacked LSTM (Units: 256, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 12.19709\n",
      "MAE: 9.34712\n",
      "R-squared: 0.92175\n",
      "SMAPE: 0.38482\n",
      "Training Time: 135.17298 seconds\n",
      "CPU Usage: 74.3 MHz\n",
      "Memory Used: 16154.52734375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 8, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 67.79653\n",
      "MAE: 57.25400\n",
      "R-squared: -1.41760\n",
      "SMAPE: 0.55775\n",
      "Training Time: 25.10791 seconds\n",
      "CPU Usage: 39.2 MHz\n",
      "Memory Used: 16162.234375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 8, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 64.41977\n",
      "MAE: 54.16244\n",
      "R-squared: -1.18277\n",
      "SMAPE: 0.53462\n",
      "Training Time: 25.83386 seconds\n",
      "CPU Usage: 38.7 MHz\n",
      "Memory Used: 16173.54296875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 32, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 23.66363\n",
      "MAE: 18.13943\n",
      "R-squared: 0.70547\n",
      "SMAPE: 0.39575\n",
      "Training Time: 26.20748 seconds\n",
      "CPU Usage: 35.4 MHz\n",
      "Memory Used: 16166.2265625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 32, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 22.31593\n",
      "MAE: 17.15359\n",
      "R-squared: 0.73806\n",
      "SMAPE: 0.38980\n",
      "Training Time: 36.17532 seconds\n",
      "CPU Usage: 36.0 MHz\n",
      "Memory Used: 16208.4609375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 64, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 14.16669\n",
      "MAE: 10.88697\n",
      "R-squared: 0.89444\n",
      "SMAPE: 0.38778\n",
      "Training Time: 21.74245 seconds\n",
      "CPU Usage: 33.3 MHz\n",
      "Memory Used: 16224.8828125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 64, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 15.70392\n",
      "MAE: 12.19033\n",
      "R-squared: 0.87029\n",
      "SMAPE: 0.38571\n",
      "Training Time: 30.09677 seconds\n",
      "CPU Usage: 37.7 MHz\n",
      "Memory Used: 16352.1953125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 10.75368\n",
      "MAE: 8.77092\n",
      "R-squared: 0.93917\n",
      "SMAPE: 0.39323\n",
      "Training Time: 41.26078 seconds\n",
      "CPU Usage: 45.9 MHz\n",
      "Memory Used: 16372.23046875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 14.13615\n",
      "MAE: 11.09804\n",
      "R-squared: 0.89489\n",
      "SMAPE: 0.38754\n",
      "Training Time: 46.10175 seconds\n",
      "CPU Usage: 45.7 MHz\n",
      "Memory Used: 16462.88671875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 5ms/step\n",
      "Model: Bidirectional LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 8.50360\n",
      "MAE: 7.11390\n",
      "R-squared: 0.96197\n",
      "SMAPE: 0.39008\n",
      "Training Time: 97.79626 seconds\n",
      "CPU Usage: 77.4 MHz\n",
      "Memory Used: 16613.70703125 MB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Bidirectional LSTM (Units: 256, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 10.57062\n",
      "MAE: 8.70269\n",
      "R-squared: 0.94123\n",
      "SMAPE: 0.39292\n",
      "Training Time: 85.88237 seconds\n",
      "CPU Usage: 70.0 MHz\n",
      "Memory Used: 16586.6171875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 8, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 82.98619\n",
      "MAE: 73.17269\n",
      "R-squared: -2.62227\n",
      "SMAPE: 0.70731\n",
      "Training Time: 19.60758 seconds\n",
      "CPU Usage: 26.6 MHz\n",
      "Memory Used: 16652.9296875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 8, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 82.20990\n",
      "MAE: 72.11019\n",
      "R-squared: -2.55482\n",
      "SMAPE: 0.69453\n",
      "Training Time: 19.94442 seconds\n",
      "CPU Usage: 27.8 MHz\n",
      "Memory Used: 16704.84765625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 32, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 28.52612\n",
      "MAE: 21.19767\n",
      "R-squared: 0.57199\n",
      "SMAPE: 0.38737\n",
      "Training Time: 20.69179 seconds\n",
      "CPU Usage: 26.0 MHz\n",
      "Memory Used: 16692.9765625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 32, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 31.67140\n",
      "MAE: 24.17123\n",
      "R-squared: 0.47240\n",
      "SMAPE: 0.39656\n",
      "Training Time: 20.82467 seconds\n",
      "CPU Usage: 26.4 MHz\n",
      "Memory Used: 16625.88671875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 64, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 16.76092\n",
      "MAE: 12.88476\n",
      "R-squared: 0.85224\n",
      "SMAPE: 0.39053\n",
      "Training Time: 20.94750 seconds\n",
      "CPU Usage: 25.0 MHz\n",
      "Memory Used: 16630.23046875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 64, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 19.85787\n",
      "MAE: 15.25364\n",
      "R-squared: 0.79259\n",
      "SMAPE: 0.38812\n",
      "Training Time: 22.04281 seconds\n",
      "CPU Usage: 26.3 MHz\n",
      "Memory Used: 16631.515625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 12.02836\n",
      "MAE: 9.23894\n",
      "R-squared: 0.92390\n",
      "SMAPE: 0.38829\n",
      "Training Time: 25.60571 seconds\n",
      "CPU Usage: 28.8 MHz\n",
      "Memory Used: 16567.18359375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: GRU (Units: 128, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 14.12001\n",
      "MAE: 11.11607\n",
      "R-squared: 0.89513\n",
      "SMAPE: 0.38305\n",
      "Training Time: 26.17646 seconds\n",
      "CPU Usage: 27.3 MHz\n",
      "Memory Used: 16580.71484375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 8.36344\n",
      "MAE: 6.86511\n",
      "R-squared: 0.96321\n",
      "SMAPE: 0.38809\n",
      "Training Time: 35.98352 seconds\n",
      "CPU Usage: 41.7 MHz\n",
      "Memory Used: 16553.328125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 256, Dropout Rate: 0.2, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 13.16893\n",
      "MAE: 10.49037\n",
      "R-squared: 0.90878\n",
      "SMAPE: 0.39250\n",
      "Training Time: 38.72532 seconds\n",
      "CPU Usage: 46.1 MHz\n",
      "Memory Used: 16550.9296875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean, gmean\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [8, 32, 64, 128, 256], 'dropout_rate': [0.0, 0.2], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Stacked LSTM': {'units': [8, 32, 64, 128, 256], 'dropout_rate': [0.0, 0.2], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Bidirectional LSTM': {'units': [8, 32, 64, 128, 256], 'dropout_rate': [0.0, 0.2], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'GRU': {'units': [8, 32, 64, 128, 256], 'dropout_rate': [0.0, 0.2], 'learning_rate': [0.001], 'batch_size': [8]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    for units in params['units']:\n",
    "        for dropout_rate in params['dropout_rate']:\n",
    "            for learning_rate in params['learning_rate']:\n",
    "                for batch_size in params['batch_size']:\n",
    "                    if model_name == 'LSTM':\n",
    "                        model = create_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Stacked LSTM':\n",
    "                        model = create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Bidirectional LSTM':\n",
    "                        model = create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'GRU':\n",
    "                        model = create_gru_model(units, dropout_rate, learning_rate, batch_size)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                        y_train,\n",
    "                                        epochs=100,\n",
    "                                        batch_size=batch_size,\n",
    "                                        verbose=0)\n",
    "                    training_time = time.time() - start_time\n",
    "\n",
    "                    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                    mse = mean_squared_error(y_test, predictions)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_test, predictions)\n",
    "                    r2 = r2_score(y_test, predictions)\n",
    "                    smape_val = smape(y_test, predictions)\n",
    "\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = model\n",
    "                        best_params = {\n",
    "                            'Units': units,\n",
    "                            'Dropout Rate': dropout_rate,\n",
    "                            'Learning Rate': learning_rate,\n",
    "                            'Batch Size': batch_size\n",
    "                        }\n",
    "\n",
    "                    print(f\"Model: {model_name} (Units: {units}, Dropout Rate: {dropout_rate}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                    print(f\"RMSE: {rmse:.5f}\")\n",
    "                    print(f\"MAE: {mae:.5f}\")\n",
    "                    print(f\"R-squared: {r2:.5f}\")\n",
    "                    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                    print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = best_model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics and best parameters in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'Best RMSE': best_rmse,\n",
    "        'Best MAE': mae,\n",
    "        'Best R-squared': r2,\n",
    "        'Best SMAPE': smape_val,\n",
    "        'Best Training Time (seconds)': training_time,\n",
    "        'Best CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Best Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the harmonic mean model\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the geometric mean model\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL_forecasts_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(hyperparameters.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': [eval_metrics[model]['Best RMSE'] for model in hyperparameters.keys()] + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': [eval_metrics[model]['Best MAE'] for model in hyperparameters.keys()] + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': [eval_metrics[model]['Best R-squared'] for model in hyperparameters.keys()] + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': [eval_metrics[model]['Best SMAPE'] for model in hyperparameters.keys()] + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': [eval_metrics[model]['Best Training Time (seconds)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'CPU Usage (MHz)': [eval_metrics[model]['Best CPU Usage (MHz)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Memory Used (MB)': [eval_metrics[model]['Best Memory Used (MB)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Best Parameters': [eval_metrics[model]['Best Parameters'] for model in hyperparameters.keys()] + [None, None, None, None]\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL6_eval_metrics_792_Months_1875_to_1941.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c505a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 14.70907\n",
      "MAE: 11.33125\n",
      "R-squared: 0.88620\n",
      "SMAPE: 0.38716\n",
      "Training Time: 23.83243 seconds\n",
      "CPU Usage: 16.0 MHz\n",
      "Memory Used: 16752.44140625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 10.66707\n",
      "MAE: 8.65575\n",
      "R-squared: 0.94015\n",
      "SMAPE: 0.39201\n",
      "Training Time: 64.89703 seconds\n",
      "CPU Usage: 55.7 MHz\n",
      "Memory Used: 16742.14453125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 13.53325\n",
      "MAE: 10.37206\n",
      "R-squared: 0.90367\n",
      "SMAPE: 0.39146\n",
      "Training Time: 42.28091 seconds\n",
      "CPU Usage: 32.6 MHz\n",
      "Memory Used: 16913.3359375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 14.94966\n",
      "MAE: 12.51886\n",
      "R-squared: 0.88245\n",
      "SMAPE: 0.40081\n",
      "Training Time: 126.20271 seconds\n",
      "CPU Usage: 77.8 MHz\n",
      "Memory Used: 16999.66796875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 11.31153\n",
      "MAE: 9.20277\n",
      "R-squared: 0.93270\n",
      "SMAPE: 0.39605\n",
      "Training Time: 37.76598 seconds\n",
      "CPU Usage: 43.9 MHz\n",
      "Memory Used: 17162.7265625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Bidirectional LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 9.22464\n",
      "MAE: 7.70732\n",
      "R-squared: 0.95524\n",
      "SMAPE: 0.39413\n",
      "Training Time: 88.76372 seconds\n",
      "CPU Usage: 72.5 MHz\n",
      "Memory Used: 17080.70703125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 11.70335\n",
      "MAE: 9.42463\n",
      "R-squared: 0.92796\n",
      "SMAPE: 0.39900\n",
      "Training Time: 21.02093 seconds\n",
      "CPU Usage: 22.2 MHz\n",
      "Memory Used: 17097.453125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: GRU (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 9.09006\n",
      "MAE: 7.40017\n",
      "R-squared: 0.95654\n",
      "SMAPE: 0.39017\n",
      "Training Time: 51.68220 seconds\n",
      "CPU Usage: 42.0 MHz\n",
      "Memory Used: 17100.015625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean, gmean\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Stacked LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Bidirectional LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'GRU': {'units': [128, 256], 'dropout_rate': [0.0,], 'learning_rate': [0.001], 'batch_size': [8]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    for units in params['units']:\n",
    "        for dropout_rate in params['dropout_rate']:\n",
    "            for learning_rate in params['learning_rate']:\n",
    "                for batch_size in params['batch_size']:\n",
    "                    if model_name == 'LSTM':\n",
    "                        model = create_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Stacked LSTM':\n",
    "                        model = create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Bidirectional LSTM':\n",
    "                        model = create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'GRU':\n",
    "                        model = create_gru_model(units, dropout_rate, learning_rate, batch_size)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                        y_train,\n",
    "                                        epochs=100,\n",
    "                                        batch_size=batch_size,\n",
    "                                        verbose=0)\n",
    "                    training_time = time.time() - start_time\n",
    "\n",
    "                    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                    mse = mean_squared_error(y_test, predictions)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_test, predictions)\n",
    "                    r2 = r2_score(y_test, predictions)\n",
    "                    smape_val = smape(y_test, predictions)\n",
    "\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = model\n",
    "                        best_params = {\n",
    "                            'Units': units,\n",
    "                            'Dropout Rate': dropout_rate,\n",
    "                            'Learning Rate': learning_rate,\n",
    "                            'Batch Size': batch_size\n",
    "                        }\n",
    "\n",
    "                    print(f\"Model: {model_name} (Units: {units}, Dropout Rate: {dropout_rate}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                    print(f\"RMSE: {rmse:.5f}\")\n",
    "                    print(f\"MAE: {mae:.5f}\")\n",
    "                    print(f\"R-squared: {r2:.5f}\")\n",
    "                    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                    print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = best_model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics and best parameters in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'Best RMSE': best_rmse,\n",
    "        'Best MAE': mae,\n",
    "        'Best R-squared': r2,\n",
    "        'Best SMAPE': smape_val,\n",
    "        'Best Training Time (seconds)': training_time,\n",
    "        'Best CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Best Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the harmonic mean model\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the geometric mean model\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL7_forecasts_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(hyperparameters.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': [eval_metrics[model]['Best RMSE'] for model in hyperparameters.keys()] + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': [eval_metrics[model]['Best MAE'] for model in hyperparameters.keys()] + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': [eval_metrics[model]['Best R-squared'] for model in hyperparameters.keys()] + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': [eval_metrics[model]['Best SMAPE'] for model in hyperparameters.keys()] + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': [eval_metrics[model]['Best Training Time (seconds)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'CPU Usage (MHz)': [eval_metrics[model]['Best CPU Usage (MHz)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Memory Used (MB)': [eval_metrics[model]['Best Memory Used (MB)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Best Parameters': [eval_metrics[model]['Best Parameters'] for model in hyperparameters.keys()] + [None, None, None, None]\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL7_eval_metrics_792_Months_1875_to_1941.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ad09dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 4.77441\n",
      "MAE: 3.73505\n",
      "R-squared: 0.98550\n",
      "SMAPE: 0.47405\n",
      "Training Time: 26.25036 seconds\n",
      "CPU Usage: 12.4 MHz\n",
      "Memory Used: 16325.8984375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 4.77488\n",
      "MAE: 3.70249\n",
      "R-squared: 0.98550\n",
      "SMAPE: 0.46071\n",
      "Training Time: 75.59181 seconds\n",
      "CPU Usage: 50.1 MHz\n",
      "Memory Used: 16313.5234375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 5.64113\n",
      "MAE: 4.17612\n",
      "R-squared: 0.97976\n",
      "SMAPE: 0.45797\n",
      "Training Time: 53.69549 seconds\n",
      "CPU Usage: 36.9 MHz\n",
      "Memory Used: 15733.30078125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Stacked LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 4.68559\n",
      "MAE: 3.53224\n",
      "R-squared: 0.98604\n",
      "SMAPE: 0.46528\n",
      "Training Time: 144.77924 seconds\n",
      "CPU Usage: 76.2 MHz\n",
      "Memory Used: 15747.03125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 5.57728\n",
      "MAE: 4.05670\n",
      "R-squared: 0.98022\n",
      "SMAPE: 0.46608\n",
      "Training Time: 49.32609 seconds\n",
      "CPU Usage: 41.5 MHz\n",
      "Memory Used: 15883.25 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Bidirectional LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 5.48513\n",
      "MAE: 4.52873\n",
      "R-squared: 0.98087\n",
      "SMAPE: 0.46674\n",
      "Training Time: 109.33292 seconds\n",
      "CPU Usage: 74.1 MHz\n",
      "Memory Used: 15944.2421875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 4.68228\n",
      "MAE: 3.67800\n",
      "R-squared: 0.98606\n",
      "SMAPE: 0.46432\n",
      "Training Time: 28.96651 seconds\n",
      "CPU Usage: 27.2 MHz\n",
      "Memory Used: 15997.0625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 5.59247\n",
      "MAE: 4.44030\n",
      "R-squared: 0.98011\n",
      "SMAPE: 0.47257\n",
      "Training Time: 53.49421 seconds\n",
      "CPU Usage: 43.7 MHz\n",
      "Memory Used: 16008.875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean, gmean\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1845 to 1931)\n",
    "filtered_data = data[(data['Year'] >= 1845) & (data['Year'] <= 1931)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 840  # Number of months for training\n",
    "test_size = 84  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Stacked LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Bidirectional LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'GRU': {'units': [128, 256], 'dropout_rate': [0.0,], 'learning_rate': [0.001], 'batch_size': [8]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    for units in params['units']:\n",
    "        for dropout_rate in params['dropout_rate']:\n",
    "            for learning_rate in params['learning_rate']:\n",
    "                for batch_size in params['batch_size']:\n",
    "                    if model_name == 'LSTM':\n",
    "                        model = create_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Stacked LSTM':\n",
    "                        model = create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Bidirectional LSTM':\n",
    "                        model = create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'GRU':\n",
    "                        model = create_gru_model(units, dropout_rate, learning_rate, batch_size)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                        y_train,\n",
    "                                        epochs=100,\n",
    "                                        batch_size=batch_size,\n",
    "                                        verbose=0)\n",
    "                    training_time = time.time() - start_time\n",
    "\n",
    "                    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                    mse = mean_squared_error(y_test, predictions)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_test, predictions)\n",
    "                    r2 = r2_score(y_test, predictions)\n",
    "                    smape_val = smape(y_test, predictions)\n",
    "\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = model\n",
    "                        best_params = {\n",
    "                            'Units': units,\n",
    "                            'Dropout Rate': dropout_rate,\n",
    "                            'Learning Rate': learning_rate,\n",
    "                            'Batch Size': batch_size\n",
    "                        }\n",
    "\n",
    "                    print(f\"Model: {model_name} (Units: {units}, Dropout Rate: {dropout_rate}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                    print(f\"RMSE: {rmse:.5f}\")\n",
    "                    print(f\"MAE: {mae:.5f}\")\n",
    "                    print(f\"R-squared: {r2:.5f}\")\n",
    "                    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                    print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = best_model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics and best parameters in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'Best RMSE': best_rmse,\n",
    "        'Best MAE': mae,\n",
    "        'Best R-squared': r2,\n",
    "        'Best SMAPE': smape_val,\n",
    "        'Best Training Time (seconds)': training_time,\n",
    "        'Best CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Best Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the harmonic mean model\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the geometric mean model\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL8_forecasts_924_Months_840tr_84t_1854_to_1931.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(hyperparameters.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': [eval_metrics[model]['Best RMSE'] for model in hyperparameters.keys()] + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': [eval_metrics[model]['Best MAE'] for model in hyperparameters.keys()] + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': [eval_metrics[model]['Best R-squared'] for model in hyperparameters.keys()] + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': [eval_metrics[model]['Best SMAPE'] for model in hyperparameters.keys()] + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': [eval_metrics[model]['Best Training Time (seconds)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'CPU Usage (MHz)': [eval_metrics[model]['Best CPU Usage (MHz)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Memory Used (MB)': [eval_metrics[model]['Best Memory Used (MB)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Best Parameters': [eval_metrics[model]['Best Parameters'] for model in hyperparameters.keys()] + [None, None, None, None]\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL8_eval_metrics_924_Months_1854_to_1931.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e391148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 4.67070\n",
      "MAE: 4.31634\n",
      "R-squared: 0.99365\n",
      "SMAPE: 0.82408\n",
      "Training Time: 26.24233 seconds\n",
      "CPU Usage: 15.5 MHz\n",
      "Memory Used: 16002.421875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 2.82235\n",
      "MAE: 2.25386\n",
      "R-squared: 0.99768\n",
      "SMAPE: 0.83699\n",
      "Training Time: 79.33321 seconds\n",
      "CPU Usage: 49.6 MHz\n",
      "Memory Used: 15930.3984375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 4.11088\n",
      "MAE: 3.41872\n",
      "R-squared: 0.99508\n",
      "SMAPE: 0.83544\n",
      "Training Time: 48.95841 seconds\n",
      "CPU Usage: 31.8 MHz\n",
      "Memory Used: 15988.08984375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Stacked LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 3.63895\n",
      "MAE: 2.86056\n",
      "R-squared: 0.99614\n",
      "SMAPE: 0.83981\n",
      "Training Time: 143.82529 seconds\n",
      "CPU Usage: 75.8 MHz\n",
      "Memory Used: 16018.953125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 3.13988\n",
      "MAE: 2.38422\n",
      "R-squared: 0.99713\n",
      "SMAPE: 0.83153\n",
      "Training Time: 42.15374 seconds\n",
      "CPU Usage: 36.6 MHz\n",
      "Memory Used: 16045.203125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 5ms/step\n",
      "Model: Bidirectional LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 3.76298\n",
      "MAE: 3.47550\n",
      "R-squared: 0.99588\n",
      "SMAPE: 0.82665\n",
      "Training Time: 104.15966 seconds\n",
      "CPU Usage: 72.4 MHz\n",
      "Memory Used: 16175.2734375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 3.04213\n",
      "MAE: 2.18887\n",
      "R-squared: 0.99730\n",
      "SMAPE: 0.84600\n",
      "Training Time: 29.50459 seconds\n",
      "CPU Usage: 27.7 MHz\n",
      "Memory Used: 16180.98046875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 4.05095\n",
      "MAE: 3.27280\n",
      "R-squared: 0.99522\n",
      "SMAPE: 0.83844\n",
      "Training Time: 58.16242 seconds\n",
      "CPU Usage: 45.2 MHz\n",
      "Memory Used: 16181.32421875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean, gmean\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1931 to 2008)\n",
    "filtered_data = data[(data['Year'] >= 1931) & (data['Year'] <= 2008)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 840  # Number of months for training\n",
    "test_size = 84  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Stacked LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Bidirectional LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'GRU': {'units': [128, 256], 'dropout_rate': [0.0,], 'learning_rate': [0.001], 'batch_size': [8]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    for units in params['units']:\n",
    "        for dropout_rate in params['dropout_rate']:\n",
    "            for learning_rate in params['learning_rate']:\n",
    "                for batch_size in params['batch_size']:\n",
    "                    if model_name == 'LSTM':\n",
    "                        model = create_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Stacked LSTM':\n",
    "                        model = create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Bidirectional LSTM':\n",
    "                        model = create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'GRU':\n",
    "                        model = create_gru_model(units, dropout_rate, learning_rate, batch_size)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                        y_train,\n",
    "                                        epochs=100,\n",
    "                                        batch_size=batch_size,\n",
    "                                        verbose=0)\n",
    "                    training_time = time.time() - start_time\n",
    "\n",
    "                    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                    mse = mean_squared_error(y_test, predictions)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_test, predictions)\n",
    "                    r2 = r2_score(y_test, predictions)\n",
    "                    smape_val = smape(y_test, predictions)\n",
    "\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = model\n",
    "                        best_params = {\n",
    "                            'Units': units,\n",
    "                            'Dropout Rate': dropout_rate,\n",
    "                            'Learning Rate': learning_rate,\n",
    "                            'Batch Size': batch_size\n",
    "                        }\n",
    "\n",
    "                    print(f\"Model: {model_name} (Units: {units}, Dropout Rate: {dropout_rate}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                    print(f\"RMSE: {rmse:.5f}\")\n",
    "                    print(f\"MAE: {mae:.5f}\")\n",
    "                    print(f\"R-squared: {r2:.5f}\")\n",
    "                    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                    print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = best_model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics and best parameters in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'Best RMSE': best_rmse,\n",
    "        'Best MAE': mae,\n",
    "        'Best R-squared': r2,\n",
    "        'Best SMAPE': smape_val,\n",
    "        'Best Training Time (seconds)': training_time,\n",
    "        'Best CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Best Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the harmonic mean model\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the geometric mean model\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL9_forecasts_924_Months_840tr_84t_1931_to_2008.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(hyperparameters.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': [eval_metrics[model]['Best RMSE'] for model in hyperparameters.keys()] + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': [eval_metrics[model]['Best MAE'] for model in hyperparameters.keys()] + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': [eval_metrics[model]['Best R-squared'] for model in hyperparameters.keys()] + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': [eval_metrics[model]['Best SMAPE'] for model in hyperparameters.keys()] + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': [eval_metrics[model]['Best Training Time (seconds)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'CPU Usage (MHz)': [eval_metrics[model]['Best CPU Usage (MHz)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Memory Used (MB)': [eval_metrics[model]['Best Memory Used (MB)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Best Parameters': [eval_metrics[model]['Best Parameters'] for model in hyperparameters.keys()] + [None, None, None, None]\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL9_eval_metrics_924_Months_1931_to_2008.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba22d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b89c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 14.51358\n",
      "MAE: 11.34709\n",
      "R-squared: 0.88921\n",
      "SMAPE: 0.39203\n",
      "Training Time: 42.55742 seconds\n",
      "CPU Usage: 43.6 MHz\n",
      "Memory Used: 8263.48046875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 10.83939\n",
      "MAE: 8.85193\n",
      "R-squared: 0.93820\n",
      "SMAPE: 0.39453\n",
      "Training Time: 75.23962 seconds\n",
      "CPU Usage: 69.8 MHz\n",
      "Memory Used: 8363.94140625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 10ms/step\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Stacked LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 13.08240\n",
      "MAE: 9.86323\n",
      "R-squared: 0.90998\n",
      "SMAPE: 0.38343\n",
      "Training Time: 44.58561 seconds\n",
      "CPU Usage: 47.2 MHz\n",
      "Memory Used: 8396.31640625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 9ms/step\n",
      "Model: Stacked LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 13.03676\n",
      "MAE: 10.55378\n",
      "R-squared: 0.91061\n",
      "SMAPE: 0.40048\n",
      "Training Time: 129.93918 seconds\n",
      "CPU Usage: 77.6 MHz\n",
      "Memory Used: 8532.953125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 12.75457\n",
      "MAE: 10.39746\n",
      "R-squared: 0.91443\n",
      "SMAPE: 0.39974\n",
      "Training Time: 32.53777 seconds\n",
      "CPU Usage: 48.7 MHz\n",
      "Memory Used: 8522.5859375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: Bidirectional LSTM (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 10.62023\n",
      "MAE: 8.75925\n",
      "R-squared: 0.94068\n",
      "SMAPE: 0.39777\n",
      "Training Time: 90.96198 seconds\n",
      "CPU Usage: 78.6 MHz\n",
      "Memory Used: 8601.4921875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "Model: GRU (Units: 128, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 13.34104\n",
      "MAE: 10.69143\n",
      "R-squared: 0.90638\n",
      "SMAPE: 0.39940\n",
      "Training Time: 26.04206 seconds\n",
      "CPU Usage: 40.0 MHz\n",
      "Memory Used: 8549.02734375 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Model: GRU (Units: 256, Dropout Rate: 0.0, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 11.56052\n",
      "MAE: 9.34486\n",
      "R-squared: 0.92970\n",
      "SMAPE: 0.39624\n",
      "Training Time: 37.32675 seconds\n",
      "CPU Usage: 51.5 MHz\n",
      "Memory Used: 8576.26953125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean, gmean\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, dropout_rate, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Stacked LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Bidirectional LSTM': {'units': [128, 256], 'dropout_rate': [0.0], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'GRU': {'units': [128, 256], 'dropout_rate': [0.0,], 'learning_rate': [0.001], 'batch_size': [8]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    for units in params['units']:\n",
    "        for dropout_rate in params['dropout_rate']:\n",
    "            for learning_rate in params['learning_rate']:\n",
    "                for batch_size in params['batch_size']:\n",
    "                    if model_name == 'LSTM':\n",
    "                        model = create_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Stacked LSTM':\n",
    "                        model = create_stacked_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'Bidirectional LSTM':\n",
    "                        model = create_bidirectional_lstm_model(units, dropout_rate, learning_rate, batch_size)\n",
    "                    elif model_name == 'GRU':\n",
    "                        model = create_gru_model(units, dropout_rate, learning_rate, batch_size)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                        y_train,\n",
    "                                        epochs=100,\n",
    "                                        batch_size=batch_size,\n",
    "                                        verbose=0)\n",
    "                    training_time = time.time() - start_time\n",
    "\n",
    "                    predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                    mse = mean_squared_error(y_test, predictions)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_test, predictions)\n",
    "                    r2 = r2_score(y_test, predictions)\n",
    "                    smape_val = smape(y_test, predictions)\n",
    "\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = model\n",
    "                        best_params = {\n",
    "                            'Units': units,\n",
    "                            'Dropout Rate': dropout_rate,\n",
    "                            'Learning Rate': learning_rate,\n",
    "                            'Batch Size': batch_size\n",
    "                        }\n",
    "\n",
    "                    print(f\"Model: {model_name} (Units: {units}, Dropout Rate: {dropout_rate}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                    print(f\"RMSE: {rmse:.5f}\")\n",
    "                    print(f\"MAE: {mae:.5f}\")\n",
    "                    print(f\"R-squared: {r2:.5f}\")\n",
    "                    print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                    print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                    print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                    print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                    print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = best_model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics and best parameters in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'Best RMSE': best_rmse,\n",
    "        'Best MAE': mae,\n",
    "        'Best R-squared': r2,\n",
    "        'Best SMAPE': smape_val,\n",
    "        'Best Training Time (seconds)': training_time,\n",
    "        'Best CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Best Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "harmonic_mean_predictions = hmean(list(model_data.values()))\n",
    "geometric_mean_predictions = gmean(list(model_data.values()))\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the harmonic mean model\n",
    "harmonic_mean_mse = mean_squared_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_rmse = np.sqrt(harmonic_mean_mse)\n",
    "harmonic_mean_mae = mean_absolute_error(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_r2 = r2_score(y_test, harmonic_mean_predictions)\n",
    "harmonic_mean_smape = smape(y_test, harmonic_mean_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the geometric mean model\n",
    "geometric_mean_mse = mean_squared_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_rmse = np.sqrt(geometric_mean_mse)\n",
    "geometric_mean_mae = mean_absolute_error(y_test, geometric_mean_predictions)\n",
    "geometric_mean_r2 = r2_score(y_test, geometric_mean_predictions)\n",
    "geometric_mean_smape = smape(y_test, geometric_mean_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('DL10_forecasts_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n",
    "\n",
    "# Prepare the data for the evaluation metrics table\n",
    "metrics_data = {\n",
    "    'Model': list(hyperparameters.keys()) + ['Combined Model', 'Median Model', 'Harmonic Mean Model', 'Geometric Mean Model'],\n",
    "    'RMSE': [eval_metrics[model]['Best RMSE'] for model in hyperparameters.keys()] + [combined_rmse, median_rmse, harmonic_mean_rmse, geometric_mean_rmse],\n",
    "    'MAE': [eval_metrics[model]['Best MAE'] for model in hyperparameters.keys()] + [combined_mae, median_mae, harmonic_mean_mae, geometric_mean_mae],\n",
    "    'R-squared': [eval_metrics[model]['Best R-squared'] for model in hyperparameters.keys()] + [combined_r2, median_r2, harmonic_mean_r2, geometric_mean_r2],\n",
    "    'SMAPE': [eval_metrics[model]['Best SMAPE'] for model in hyperparameters.keys()] + [combined_smape, median_smape, harmonic_mean_smape, geometric_mean_smape],\n",
    "    'Training Time (seconds)': [eval_metrics[model]['Best Training Time (seconds)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'CPU Usage (MHz)': [eval_metrics[model]['Best CPU Usage (MHz)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Memory Used (MB)': [eval_metrics[model]['Best Memory Used (MB)'] for model in hyperparameters.keys()] + [None, None, None, None],\n",
    "    'Best Parameters': [eval_metrics[model]['Best Parameters'] for model in hyperparameters.keys()] + [None, None, None, None]\n",
    "}\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df.to_csv('DL10_eval_metrics_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98b7c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: LSTM (Units: 128, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 16.76721\n",
      "MAE: 13.12013\n",
      "R-squared: 0.85213\n",
      "SMAPE: 0.39016\n",
      "Training Time: 26.84632 seconds\n",
      "CPU Usage: 25.3 MHz\n",
      "Memory Used: 13020.5 MB\n",
      "\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 128, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 15.51223\n",
      "MAE: 12.08561\n",
      "R-squared: 0.87343\n",
      "SMAPE: 0.37858\n",
      "Training Time: 52.40342 seconds\n",
      "CPU Usage: 34.8 MHz\n",
      "Memory Used: 13056.6640625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 15.57322\n",
      "MAE: 12.59646\n",
      "R-squared: 0.87244\n",
      "SMAPE: 0.39772\n",
      "Training Time: 38.10903 seconds\n",
      "CPU Usage: 34.7 MHz\n",
      "Memory Used: 13126.8046875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Model: GRU (Units: 128, Learning Rate: 0.001, Batch Size: 8)\n",
      "RMSE: 14.88445\n",
      "MAE: 11.42647\n",
      "R-squared: 0.88347\n",
      "SMAPE: 0.38509\n",
      "Training Time: 27.87644 seconds\n",
      "CPU Usage: 26.5 MHz\n",
      "Memory Used: 13176.11328125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "True Values and Forecasts:\n",
      "    True Value  True Value        LSTM  Stacked LSTM  Bidirectional LSTM  \\\n",
      "0         32.6        32.6   29.124769     30.364592           29.046431   \n",
      "1         36.7        36.7   32.228252     33.854546           32.161327   \n",
      "2         42.7        42.7   36.014450     38.187634           35.952797   \n",
      "3         49.8        49.8   41.478600     44.521164           41.391724   \n",
      "4         56.9        56.9   47.967739     52.027023           47.776623   \n",
      "..         ...         ...         ...           ...                 ...   \n",
      "66       111.3       111.3  115.022446    120.757294          109.851494   \n",
      "67       107.7       107.7  113.696808    119.388756          108.587852   \n",
      "68       103.2       103.2  109.935837    115.549889          105.043945   \n",
      "69        99.5        99.5  105.071136    110.664062          100.534225   \n",
      "70        96.1        96.1  100.962036    106.587425           96.773712   \n",
      "\n",
      "           GRU  \n",
      "0    30.584869  \n",
      "1    34.004105  \n",
      "2    38.227646  \n",
      "3    44.344131  \n",
      "4    51.471508  \n",
      "..         ...  \n",
      "66  115.233528  \n",
      "67  113.926971  \n",
      "68  110.259346  \n",
      "69  105.596634  \n",
      "70  101.722984  \n",
      "\n",
      "[71 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, learning_rate, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [128], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Stacked LSTM': {'units': [128], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'Bidirectional LSTM': {'units': [128], 'learning_rate': [0.001], 'batch_size': [8]},\n",
    "    'GRU': {'units': [128], 'learning_rate': [0.001], 'batch_size': [8]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for units in params['units']:\n",
    "        for learning_rate in params['learning_rate']:\n",
    "            for batch_size in params['batch_size']:\n",
    "                if model_name == 'LSTM':\n",
    "                    model = create_lstm_model(units, learning_rate, batch_size)\n",
    "                elif model_name == 'Stacked LSTM':\n",
    "                    model = create_stacked_lstm_model(units, learning_rate, batch_size)\n",
    "                elif model_name == 'Bidirectional LSTM':\n",
    "                    model = create_bidirectional_lstm_model(units, learning_rate, batch_size)\n",
    "                elif model_name == 'GRU':\n",
    "                    model = create_gru_model(units, learning_rate, batch_size)\n",
    "\n",
    "                start_time = time.time()\n",
    "                history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                    y_train,\n",
    "                                    epochs=100,\n",
    "                                    batch_size=batch_size,\n",
    "                                    verbose=0)\n",
    "                training_time = time.time() - start_time\n",
    "\n",
    "                predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "                mse = mean_squared_error(y_test, predictions)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(y_test, predictions)\n",
    "                r2 = r2_score(y_test, predictions)\n",
    "                smape_val = smape(y_test, predictions)\n",
    "\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = model\n",
    "\n",
    "                print(f\"Model: {model_name} (Units: {units}, Learning Rate: {learning_rate}, Batch Size: {batch_size})\")\n",
    "                print(f\"RMSE: {rmse:.5f}\")\n",
    "                print(f\"MAE: {mae:.5f}\")\n",
    "                print(f\"R-squared: {r2:.5f}\")\n",
    "                print(f\"SMAPE: {smape_val:.5f}\")\n",
    "                print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "                print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "                print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "                print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[model_name] = best_model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'RMSE': best_rmse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2,\n",
    "        'SMAPE': smape_val,\n",
    "        'Training Time (seconds)': training_time,\n",
    "        'CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024\n",
    "    }\n",
    "\n",
    "# Combine the predictions of the best models\n",
    "combined_predictions = np.mean(list(model_data.values()), axis=0)\n",
    "median_predictions = np.median(list(model_data.values()), axis=0)\n",
    "\n",
    "# Compute evaluation metrics for the combined model\n",
    "combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "combined_rmse = np.sqrt(combined_mse)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "combined_r2 = r2_score(y_test, combined_predictions)\n",
    "combined_smape = smape(y_test, combined_predictions)\n",
    "\n",
    "# Compute evaluation metrics for the median model\n",
    "median_mse = mean_squared_error(y_test, median_predictions)\n",
    "median_rmse = np.sqrt(median_mse)\n",
    "median_mae = mean_absolute_error(y_test, median_predictions)\n",
    "median_r2 = r2_score(y_test, median_predictions)\n",
    "median_smape = smape(y_test, median_predictions)\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('forecasts_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n",
    "\n",
    "# Save evaluation metrics to a CSV file\n",
    "df_metrics = pd.DataFrame(eval_metrics).T\n",
    "df_metrics.to_csv('DL_eval_metrics_792_Months_1875_to_1941.csv')\n",
    "\n",
    "# Print true values and forecasts in table format\n",
    "df_true_values = pd.DataFrame(y_test, columns=['True Value'])\n",
    "df_forecasts = pd.DataFrame(model_data)\n",
    "\n",
    "print(\"True Values and Forecasts:\")\n",
    "print(pd.concat([df_true_values, df_forecasts], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d8fab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: LSTM (Units: 32, Dropout Rate: 0.0)\n",
      "RMSE: 97.99311\n",
      "MAE: 88.53317\n",
      "R-squared: -4.05081\n",
      "SMAPE: 0.89003\n",
      "Training Time: 12.47534 seconds\n",
      "CPU Usage: 34.6 MHz\n",
      "Memory Used: 14601.671875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 2ms/step\n",
      "Model: LSTM (Units: 32, Dropout Rate: 0.2)\n",
      "RMSE: 97.43258\n",
      "MAE: 87.97609\n",
      "R-squared: -3.99319\n",
      "SMAPE: 0.88257\n",
      "Training Time: 12.35956 seconds\n",
      "CPU Usage: 41.6 MHz\n",
      "Memory Used: 14647.19921875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: LSTM (Units: 32, Dropout Rate: 0.4)\n",
      "RMSE: 95.26366\n",
      "MAE: 85.71160\n",
      "R-squared: -3.77336\n",
      "SMAPE: 0.85180\n",
      "Training Time: 12.30328 seconds\n",
      "CPU Usage: 45.4 MHz\n",
      "Memory Used: 14718.7734375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: LSTM (Units: 64, Dropout Rate: 0.0)\n",
      "RMSE: 66.57993\n",
      "MAE: 56.11668\n",
      "R-squared: -1.33161\n",
      "SMAPE: 0.54892\n",
      "Training Time: 12.75124 seconds\n",
      "CPU Usage: 43.8 MHz\n",
      "Memory Used: 14721.24609375 MB\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BF84DE2430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 1s 5ms/step\n",
      "Model: LSTM (Units: 64, Dropout Rate: 0.2)\n",
      "RMSE: 68.83334\n",
      "MAE: 58.52276\n",
      "R-squared: -1.49211\n",
      "SMAPE: 0.56906\n",
      "Training Time: 13.23047 seconds\n",
      "CPU Usage: 50.6 MHz\n",
      "Memory Used: 14850.0234375 MB\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BF924368B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: LSTM (Units: 64, Dropout Rate: 0.4)\n",
      "RMSE: 68.53484\n",
      "MAE: 58.05651\n",
      "R-squared: -1.47054\n",
      "SMAPE: 0.56217\n",
      "Training Time: 13.80575 seconds\n",
      "CPU Usage: 41.4 MHz\n",
      "Memory Used: 14942.02734375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 5ms/step\n",
      "Model: LSTM (Units: 128, Dropout Rate: 0.0)\n",
      "RMSE: 45.16266\n",
      "MAE: 35.22058\n",
      "R-squared: -0.07282\n",
      "SMAPE: 0.41914\n",
      "Training Time: 16.15266 seconds\n",
      "CPU Usage: 39.8 MHz\n",
      "Memory Used: 15040.94140625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: LSTM (Units: 128, Dropout Rate: 0.2)\n",
      "RMSE: 47.28879\n",
      "MAE: 37.18522\n",
      "R-squared: -0.17621\n",
      "SMAPE: 0.42751\n",
      "Training Time: 17.29864 seconds\n",
      "CPU Usage: 56.7 MHz\n",
      "Memory Used: 14971.9921875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: LSTM (Units: 128, Dropout Rate: 0.4)\n",
      "RMSE: 44.70526\n",
      "MAE: 35.06289\n",
      "R-squared: -0.05120\n",
      "SMAPE: 0.42058\n",
      "Training Time: 15.51187 seconds\n",
      "CPU Usage: 43.6 MHz\n",
      "Memory Used: 14981.2890625 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 1s 5ms/step\n",
      "Model: Stacked LSTM (Units: 32, Dropout Rate: 0.0)\n",
      "RMSE: 92.78572\n",
      "MAE: 83.35884\n",
      "R-squared: -3.52827\n",
      "SMAPE: 0.82364\n",
      "Training Time: 18.54950 seconds\n",
      "CPU Usage: 42.3 MHz\n",
      "Memory Used: 14996.109375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 9ms/step\n",
      "Model: Stacked LSTM (Units: 32, Dropout Rate: 0.2)\n",
      "RMSE: 92.58956\n",
      "MAE: 83.15576\n",
      "R-squared: -3.50914\n",
      "SMAPE: 0.82102\n",
      "Training Time: 20.42981 seconds\n",
      "CPU Usage: 44.2 MHz\n",
      "Memory Used: 14948.76171875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 9ms/step\n",
      "Model: Stacked LSTM (Units: 32, Dropout Rate: 0.4)\n",
      "RMSE: 92.88203\n",
      "MAE: 83.29177\n",
      "R-squared: -3.53767\n",
      "SMAPE: 0.82210\n",
      "Training Time: 20.81856 seconds\n",
      "CPU Usage: 39.6 MHz\n",
      "Memory Used: 14964.46484375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: Stacked LSTM (Units: 64, Dropout Rate: 0.0)\n",
      "RMSE: 66.02414\n",
      "MAE: 55.51149\n",
      "R-squared: -1.29285\n",
      "SMAPE: 0.54441\n",
      "Training Time: 20.81440 seconds\n",
      "CPU Usage: 40.7 MHz\n",
      "Memory Used: 14999.28125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 10ms/step\n",
      "Model: Stacked LSTM (Units: 64, Dropout Rate: 0.2)\n",
      "RMSE: 64.87415\n",
      "MAE: 54.93561\n",
      "R-squared: -1.21367\n",
      "SMAPE: 0.54390\n",
      "Training Time: 24.05440 seconds\n",
      "CPU Usage: 39.2 MHz\n",
      "Memory Used: 15002.73828125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 10ms/step\n",
      "Model: Stacked LSTM (Units: 64, Dropout Rate: 0.4)\n",
      "RMSE: 67.15915\n",
      "MAE: 57.08199\n",
      "R-squared: -1.37236\n",
      "SMAPE: 0.56005\n",
      "Training Time: 23.76521 seconds\n",
      "CPU Usage: 41.0 MHz\n",
      "Memory Used: 15025.1015625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 6ms/step\n",
      "Model: Stacked LSTM (Units: 128, Dropout Rate: 0.0)\n",
      "RMSE: 42.78693\n",
      "MAE: 32.91786\n",
      "R-squared: 0.03708\n",
      "SMAPE: 0.40948\n",
      "Training Time: 35.69195 seconds\n",
      "CPU Usage: 46.8 MHz\n",
      "Memory Used: 15109.6640625 MB\n",
      "\n",
      "3/3 [==============================] - 3s 8ms/step\n",
      "Model: Stacked LSTM (Units: 128, Dropout Rate: 0.2)\n",
      "RMSE: 42.61321\n",
      "MAE: 32.76720\n",
      "R-squared: 0.04488\n",
      "SMAPE: 0.41020\n",
      "Training Time: 37.09436 seconds\n",
      "CPU Usage: 45.8 MHz\n",
      "Memory Used: 15139.07421875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Stacked LSTM (Units: 128, Dropout Rate: 0.4)\n",
      "RMSE: 44.07683\n",
      "MAE: 34.11446\n",
      "R-squared: -0.02186\n",
      "SMAPE: 0.41487\n",
      "Training Time: 36.06558 seconds\n",
      "CPU Usage: 45.7 MHz\n",
      "Memory Used: 15270.8671875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 2s 4ms/step\n",
      "Model: Bidirectional LSTM (Units: 32, Dropout Rate: 0.0)\n",
      "RMSE: 73.54234\n",
      "MAE: 63.34094\n",
      "R-squared: -1.84475\n",
      "SMAPE: 0.61075\n",
      "Training Time: 13.16951 seconds\n",
      "CPU Usage: 46.7 MHz\n",
      "Memory Used: 15562.984375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 10ms/step\n",
      "Model: Bidirectional LSTM (Units: 32, Dropout Rate: 0.2)\n",
      "RMSE: 72.34151\n",
      "MAE: 61.88995\n",
      "R-squared: -1.75261\n",
      "SMAPE: 0.59457\n",
      "Training Time: 15.66380 seconds\n",
      "CPU Usage: 36.6 MHz\n",
      "Memory Used: 15521.3203125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: Bidirectional LSTM (Units: 32, Dropout Rate: 0.4)\n",
      "RMSE: 71.26038\n",
      "MAE: 61.03555\n",
      "R-squared: -1.67095\n",
      "SMAPE: 0.59004\n",
      "Training Time: 16.98364 seconds\n",
      "CPU Usage: 31.0 MHz\n",
      "Memory Used: 15488.7578125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: Bidirectional LSTM (Units: 64, Dropout Rate: 0.0)\n",
      "RMSE: 46.56490\n",
      "MAE: 36.73369\n",
      "R-squared: -0.14048\n",
      "SMAPE: 0.42703\n",
      "Training Time: 17.46941 seconds\n",
      "CPU Usage: 35.7 MHz\n",
      "Memory Used: 15478.1171875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: Bidirectional LSTM (Units: 64, Dropout Rate: 0.2)\n",
      "RMSE: 46.89874\n",
      "MAE: 36.63908\n",
      "R-squared: -0.15689\n",
      "SMAPE: 0.42311\n",
      "Training Time: 19.24244 seconds\n",
      "CPU Usage: 33.4 MHz\n",
      "Memory Used: 15476.5234375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 6ms/step\n",
      "Model: Bidirectional LSTM (Units: 64, Dropout Rate: 0.4)\n",
      "RMSE: 48.26586\n",
      "MAE: 38.57485\n",
      "R-squared: -0.22532\n",
      "SMAPE: 0.43711\n",
      "Training Time: 18.12246 seconds\n",
      "CPU Usage: 34.8 MHz\n",
      "Memory Used: 15484.5234375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Dropout Rate: 0.0)\n",
      "RMSE: 30.54392\n",
      "MAE: 23.20203\n",
      "R-squared: 0.50930\n",
      "SMAPE: 0.38969\n",
      "Training Time: 23.54903 seconds\n",
      "CPU Usage: 47.3 MHz\n",
      "Memory Used: 15523.58203125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 9ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Dropout Rate: 0.2)\n",
      "RMSE: 31.43808\n",
      "MAE: 24.50344\n",
      "R-squared: 0.48015\n",
      "SMAPE: 0.40117\n",
      "Training Time: 25.71166 seconds\n",
      "CPU Usage: 51.4 MHz\n",
      "Memory Used: 15563.1171875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Model: Bidirectional LSTM (Units: 128, Dropout Rate: 0.4)\n",
      "RMSE: 31.29133\n",
      "MAE: 24.30813\n",
      "R-squared: 0.48499\n",
      "SMAPE: 0.39923\n",
      "Training Time: 26.42163 seconds\n",
      "CPU Usage: 47.8 MHz\n",
      "Memory Used: 15558.53515625 MB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 10ms/step\n",
      "3/3 [==============================] - 1s 6ms/step\n",
      "Model: GRU (Units: 32, Dropout Rate: 0.0)\n",
      "RMSE: 85.15697\n",
      "MAE: 75.28430\n",
      "R-squared: -2.81426\n",
      "SMAPE: 0.72890\n",
      "Training Time: 12.40072 seconds\n",
      "CPU Usage: 44.7 MHz\n",
      "Memory Used: 15557.17578125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: GRU (Units: 32, Dropout Rate: 0.2)\n",
      "RMSE: 83.15985\n",
      "MAE: 73.19826\n",
      "R-squared: -2.63745\n",
      "SMAPE: 0.70633\n",
      "Training Time: 12.58051 seconds\n",
      "CPU Usage: 33.3 MHz\n",
      "Memory Used: 15575.30859375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 5ms/step\n",
      "Model: GRU (Units: 32, Dropout Rate: 0.4)\n",
      "RMSE: 86.92422\n",
      "MAE: 77.14907\n",
      "R-squared: -2.97421\n",
      "SMAPE: 0.74901\n",
      "Training Time: 13.60795 seconds\n",
      "CPU Usage: 36.8 MHz\n",
      "Memory Used: 15589.1875 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: GRU (Units: 64, Dropout Rate: 0.0)\n",
      "RMSE: 60.73946\n",
      "MAE: 49.98301\n",
      "R-squared: -0.94049\n",
      "SMAPE: 0.50175\n",
      "Training Time: 13.36260 seconds\n",
      "CPU Usage: 32.8 MHz\n",
      "Memory Used: 15594.65234375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 0s/step\n",
      "Model: GRU (Units: 64, Dropout Rate: 0.2)\n",
      "RMSE: 59.29765\n",
      "MAE: 49.00677\n",
      "R-squared: -0.84946\n",
      "SMAPE: 0.49753\n",
      "Training Time: 14.02671 seconds\n",
      "CPU Usage: 29.1 MHz\n",
      "Memory Used: 15634.93359375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: GRU (Units: 64, Dropout Rate: 0.4)\n",
      "RMSE: 57.04519\n",
      "MAE: 46.35457\n",
      "R-squared: -0.71162\n",
      "SMAPE: 0.47662\n",
      "Training Time: 13.77804 seconds\n",
      "CPU Usage: 34.7 MHz\n",
      "Memory Used: 15620.97265625 MB\n",
      "\n",
      "3/3 [==============================] - 1s 3ms/step\n",
      "Model: GRU (Units: 128, Dropout Rate: 0.0)\n",
      "RMSE: 37.79406\n",
      "MAE: 28.99699\n",
      "R-squared: 0.24869\n",
      "SMAPE: 0.40338\n",
      "Training Time: 17.36082 seconds\n",
      "CPU Usage: 41.5 MHz\n",
      "Memory Used: 15690.08203125 MB\n",
      "\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "Model: GRU (Units: 128, Dropout Rate: 0.2)\n",
      "RMSE: 37.32680\n",
      "MAE: 28.76298\n",
      "R-squared: 0.26716\n",
      "SMAPE: 0.40443\n",
      "Training Time: 18.45006 seconds\n",
      "CPU Usage: 38.4 MHz\n",
      "Memory Used: 15711.55859375 MB\n",
      "\n",
      "3/3 [==============================] - 1s 6ms/step\n",
      "Model: GRU (Units: 128, Dropout Rate: 0.4)\n",
      "RMSE: 36.63945\n",
      "MAE: 28.15530\n",
      "R-squared: 0.29390\n",
      "SMAPE: 0.40193\n",
      "Training Time: 17.27701 seconds\n",
      "CPU Usage: 42.3 MHz\n",
      "Memory Used: 15708.51953125 MB\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "True Values and Forecasts:\n",
      "    True Value  True Value  LSTM (Units: 128, Dropout Rate: 0.4)  \\\n",
      "0         32.6        32.6                             27.217964   \n",
      "1         36.7        36.7                             30.440594   \n",
      "2         42.7        42.7                             34.722202   \n",
      "3         49.8        49.8                             41.516628   \n",
      "4         56.9        56.9                             50.184998   \n",
      "..         ...         ...                                   ...   \n",
      "66       111.3       111.3                            105.942093   \n",
      "67       107.7       107.7                            105.473877   \n",
      "68       103.2       103.2                            104.061340   \n",
      "69        99.5        99.5                            102.032913   \n",
      "70        96.1        96.1                            100.121475   \n",
      "\n",
      "    Stacked LSTM (Units: 128, Dropout Rate: 0.2)  \\\n",
      "0                                      29.771305   \n",
      "1                                      32.860004   \n",
      "2                                      36.594406   \n",
      "3                                      42.075119   \n",
      "4                                      48.899296   \n",
      "..                                           ...   \n",
      "66                                    110.929192   \n",
      "67                                    110.380318   \n",
      "68                                    108.649796   \n",
      "69                                    106.012581   \n",
      "70                                    103.419533   \n",
      "\n",
      "    Bidirectional LSTM (Units: 128, Dropout Rate: 0.0)  \\\n",
      "0                                           28.871595    \n",
      "1                                           31.933346    \n",
      "2                                           35.779636    \n",
      "3                                           41.631233    \n",
      "4                                           49.064632    \n",
      "..                                                ...    \n",
      "66                                         114.925636    \n",
      "67                                         114.060379    \n",
      "68                                         111.518120    \n",
      "69                                         108.029884    \n",
      "70                                         104.898804    \n",
      "\n",
      "    GRU (Units: 128, Dropout Rate: 0.4)  \n",
      "0                             28.935532  \n",
      "1                             32.037178  \n",
      "2                             35.854549  \n",
      "3                             41.457485  \n",
      "4                             48.327564  \n",
      "..                                  ...  \n",
      "66                           111.074234  \n",
      "67                           110.304665  \n",
      "68                           108.011528  \n",
      "69                           104.797256  \n",
      "70                           101.856224  \n",
      "\n",
      "[71 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]},\n",
    "    'Stacked LSTM': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]},\n",
    "    'Bidirectional LSTM': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]},\n",
    "    'GRU': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_units = 0\n",
    "    best_dropout_rate = 0\n",
    "\n",
    "    for units in params['units']:\n",
    "        for dropout_rate in params['dropout_rate']:\n",
    "            if model_name == 'LSTM':\n",
    "                model = create_lstm_model(units, dropout_rate)\n",
    "            elif model_name == 'Stacked LSTM':\n",
    "                model = create_stacked_lstm_model(units, dropout_rate)\n",
    "            elif model_name == 'Bidirectional LSTM':\n",
    "                model = create_bidirectional_lstm_model(units, dropout_rate)\n",
    "            elif model_name == 'GRU':\n",
    "                model = create_gru_model(units, dropout_rate)\n",
    "\n",
    "            start_time = time.time()\n",
    "            history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                y_train,\n",
    "                                epochs=100,\n",
    "                                batch_size=32,\n",
    "                                verbose=0)\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, predictions)\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            smape_val = smape(y_test, predictions)\n",
    "\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_model = model\n",
    "                best_units = units\n",
    "                best_dropout_rate = dropout_rate\n",
    "\n",
    "            print(f\"Model: {model_name} (Units: {units}, Dropout Rate: {dropout_rate})\")\n",
    "            print(f\"RMSE: {rmse:.5f}\")\n",
    "            print(f\"MAE: {mae:.5f}\")\n",
    "            print(f\"R-squared: {r2:.5f}\")\n",
    "            print(f\"SMAPE: {smape_val:.5f}\")\n",
    "            print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "            print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "            print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "            print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[f\"{model_name} (Units: {best_units}, Dropout Rate: {best_dropout_rate})\"] = best_model.predict(\n",
    "        X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'RMSE': best_rmse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2,\n",
    "        'SMAPE': smape_val,\n",
    "        'Training Time (seconds)': training_time,\n",
    "        'CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024\n",
    "    }\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('forecasts_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n",
    "\n",
    "# Save evaluation metrics to a CSV file\n",
    "df_metrics = pd.DataFrame(eval_metrics).T\n",
    "df_metrics.to_csv('DL_eval_metrics_792_Months_1875_to_1941.csv')\n",
    "\n",
    "# Print true values and forecasts in table format\n",
    "df_true_values = pd.DataFrame(y_test, columns=['True Value'])\n",
    "df_forecasts = pd.DataFrame(model_data)\n",
    "\n",
    "print(\"True Values and Forecasts:\")\n",
    "print(pd.concat([df_true_values, df_forecasts], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ba8bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 6ms/step\n",
      "Model: LSTM (Units: 128, Learning Rate: 0.001)\n",
      "RMSE: 12.88106\n",
      "MAE: 10.20476\n",
      "R-squared: 0.91273\n",
      "SMAPE: 0.38743\n",
      "Training Time: 45.72777 seconds\n",
      "CPU Usage: 15.1 MHz\n",
      "Memory Used: 15469.046875 MB\n",
      "\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "True Values and Forecasts:\n",
      "    True Value  True Value  LSTM (Units: 128, Learning Rate: 0.001)\n",
      "0         32.6        32.6                                28.873867\n",
      "1         36.7        36.7                                32.284016\n",
      "2         42.7        42.7                                36.535164\n",
      "3         49.8        49.8                                42.759125\n",
      "4         56.9        56.9                                50.098454\n",
      "..         ...         ...                                      ...\n",
      "66       111.3       111.3                               116.850945\n",
      "67       107.7       107.7                               115.463951\n",
      "68       103.2       103.2                               111.579468\n",
      "69        99.5        99.5                               106.660713\n",
      "70        96.1        96.1                               102.590355\n",
      "\n",
      "[71 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_lstm_model(units, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for the LSTM model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [128], 'learning_rate': [0.001], 'batch_size': [8]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each LSTM model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_units = 0\n",
    "    best_learning_rate = 0\n",
    "\n",
    "    for units in params['units']:\n",
    "        for learning_rate in params['learning_rate']:\n",
    "            model = create_lstm_model(units, learning_rate)\n",
    "\n",
    "            start_time = time.time()\n",
    "            history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                y_train,\n",
    "                                epochs=100,\n",
    "                                batch_size=params['batch_size'][0],\n",
    "                                verbose=0)\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, predictions)\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            smape_val = smape(y_test, predictions)\n",
    "\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_model = model\n",
    "                best_units = units\n",
    "                best_learning_rate = learning_rate\n",
    "\n",
    "            print(f\"Model: {model_name} (Units: {units}, Learning Rate: {learning_rate})\")\n",
    "            print(f\"RMSE: {rmse:.5f}\")\n",
    "            print(f\"MAE: {mae:.5f}\")\n",
    "            print(f\"R-squared: {r2:.5f}\")\n",
    "            print(f\"SMAPE: {smape_val:.5f}\")\n",
    "            print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "            print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "            print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "            print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[f\"{model_name} (Units: {best_units}, Learning Rate: {best_learning_rate})\"] = best_model.predict(\n",
    "        X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'RMSE': best_rmse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2,\n",
    "        'SMAPE': smape_val,\n",
    "        'Training Time (seconds)': training_time,\n",
    "        'CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024\n",
    "    }\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('forecasts_792_Months_720tr_72t_1875_to_1941.csv', index=False)\n",
    "\n",
    "# Save evaluation metrics to a CSV file\n",
    "df_metrics = pd.DataFrame(eval_metrics).T\n",
    "df_metrics.to_csv('eval_metrics_792_Months_1875_to_1941.csv')\n",
    "\n",
    "# Print true values and forecasts in table format\n",
    "df_true_values = pd.DataFrame(y_test, columns=['True Value'])\n",
    "df_forecasts = pd.DataFrame(model_data)\n",
    "\n",
    "print(\"True Values and Forecasts:\")\n",
    "print(pd.concat([df_true_values, df_forecasts], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6ee51",
   "metadata": {},
   "source": [
    "# Deep learning 924 MONTHS (840 MONTHS training, 84 Testing) - Interval (1854-1931)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b46ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1931 to 2008)\n",
    "filtered_data = data[(data['Year'] >= 1931) & (data['Year'] <= 2008)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]},\n",
    "    'Stacked LSTM': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]},\n",
    "    'Bidirectional LSTM': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]},\n",
    "    'GRU': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_units = 0\n",
    "    best_dropout_rate = 0\n",
    "\n",
    "    for units in params['units']:\n",
    "        for dropout_rate in params['dropout_rate']:\n",
    "            if model_name == 'LSTM':\n",
    "                model = create_lstm_model(units, dropout_rate)\n",
    "            elif model_name == 'Stacked LSTM':\n",
    "                model = create_stacked_lstm_model(units, dropout_rate)\n",
    "            elif model_name == 'Bidirectional LSTM':\n",
    "                model = create_bidirectional_lstm_model(units, dropout_rate)\n",
    "            elif model_name == 'GRU':\n",
    "                model = create_gru_model(units, dropout_rate)\n",
    "\n",
    "            start_time = time.time()\n",
    "            history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                y_train,\n",
    "                                epochs=100,\n",
    "                                batch_size=32,\n",
    "                                verbose=0)\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, predictions)\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            smape_val = smape(y_test, predictions)\n",
    "\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_model = model\n",
    "                best_units = units\n",
    "                best_dropout_rate = dropout_rate\n",
    "\n",
    "            print(f\"Model: {model_name} (Units: {units}, Dropout Rate: {dropout_rate})\")\n",
    "            print(f\"RMSE: {rmse:.5f}\")\n",
    "            print(f\"MAE: {mae:.5f}\")\n",
    "            print(f\"R-squared: {r2:.5f}\")\n",
    "            print(f\"SMAPE: {smape_val:.5f}\")\n",
    "            print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "            print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "            print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "            print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[f\"{model_name} (Units: {best_units}, Dropout Rate: {best_dropout_rate})\"] = best_model.predict(\n",
    "        X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'RMSE': best_rmse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2,\n",
    "        'SMAPE': smape_val,\n",
    "        'Training Time (seconds)': training_time,\n",
    "        'CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024\n",
    "    }\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('forecasts_1931_to_2008.csv', index=False)\n",
    "\n",
    "# Save evaluation metrics to a CSV file\n",
    "df_metrics = pd.DataFrame(eval_metrics).T\n",
    "df_metrics.to_csv('eval_metrics_1931_to_2008.csv')\n",
    "\n",
    "# Print true values and forecasts in table format\n",
    "df_true_values = pd.DataFrame(y_test, columns=['True Value'])\n",
    "df_forecasts = pd.DataFrame(model_data)\n",
    "\n",
    "print(\"True Values and Forecasts:\")\n",
    "print(pd.concat([df_true_values, df_forecasts], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f319ead",
   "metadata": {},
   "source": [
    "# Deep learning 924 MONTHS (840 MONTHS training, 84 Testing) - Interval (1931-2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number', 'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1931 to 2008)\n",
    "filtered_data = data[(data['Year'] >= 1931) & (data['Year'] <= 2008)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size+test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the deep learning models to be evaluated\n",
    "def create_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units), input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(units, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for each model\n",
    "hyperparameters = {\n",
    "    'LSTM': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]},\n",
    "    'Stacked LSTM': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]},\n",
    "    'Bidirectional LSTM': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]},\n",
    "    'GRU': {'units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.4]}\n",
    "}\n",
    "\n",
    "# Compute SMAPE (Symmetric Mean Absolute Percentage Error) for the models\n",
    "def smape(y_true, y_pred):\n",
    "    return 2 * np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Create a dictionary to store the forecasts and actual values for each model\n",
    "model_data = {'True Value': y_test.ravel()}\n",
    "\n",
    "# Create a dictionary to store the evaluation metrics for each model\n",
    "eval_metrics = {}\n",
    "\n",
    "# Train and evaluate each deep learning model with hyperparameter tuning\n",
    "for model_name, params in hyperparameters.items():\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_units = 0\n",
    "    best_dropout_rate = 0\n",
    "\n",
    "    for units in params['units']:\n",
    "        for dropout_rate in params['dropout_rate']:\n",
    "            if model_name == 'LSTM':\n",
    "                model = create_lstm_model(units, dropout_rate)\n",
    "            elif model_name == 'Stacked LSTM':\n",
    "                model = create_stacked_lstm_model(units, dropout_rate)\n",
    "            elif model_name == 'Bidirectional LSTM':\n",
    "                model = create_bidirectional_lstm_model(units, dropout_rate)\n",
    "            elif model_name == 'GRU':\n",
    "                model = create_gru_model(units, dropout_rate)\n",
    "\n",
    "            start_time = time.time()\n",
    "            history = model.fit(X_train.reshape((X_train.shape[0], 1, X_train.shape[1])),\n",
    "                                y_train,\n",
    "                                epochs=100,\n",
    "                                batch_size=32,\n",
    "                                verbose=0)\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            predictions = model.predict(X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, predictions)\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            smape_val = smape(y_test, predictions)\n",
    "\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_model = model\n",
    "                best_units = units\n",
    "                best_dropout_rate = dropout_rate\n",
    "\n",
    "            print(f\"Model: {model_name} (Units: {units}, Dropout Rate: {dropout_rate})\")\n",
    "            print(f\"RMSE: {rmse:.5f}\")\n",
    "            print(f\"MAE: {mae:.5f}\")\n",
    "            print(f\"R-squared: {r2:.5f}\")\n",
    "            print(f\"SMAPE: {smape_val:.5f}\")\n",
    "            print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "            print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "            print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n",
    "            print()\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    model_data[f\"{model_name} (Units: {best_units}, Dropout Rate: {best_dropout_rate})\"] = best_model.predict(\n",
    "        X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))).ravel()\n",
    "\n",
    "    # Include evaluation metrics in the eval metrics dictionary\n",
    "    eval_metrics[model_name] = {\n",
    "        'RMSE': best_rmse,\n",
    "        'MAE': mae,\n",
    "        'R-squared': r2,\n",
    "        'SMAPE': smape_val,\n",
    "        'Training Time (seconds)': training_time,\n",
    "        'CPU Usage (MHz)': psutil.cpu_percent(),\n",
    "        'Memory Used (MB)': psutil.virtual_memory().used / 1024 / 1024\n",
    "    }\n",
    "\n",
    "# Save forecasts to a CSV file\n",
    "df_data = pd.DataFrame(model_data)\n",
    "df_data.to_csv('forecasts_1931_to_2008.csv', index=False)\n",
    "\n",
    "# Save evaluation metrics to a CSV file\n",
    "df_metrics = pd.DataFrame(eval_metrics).T\n",
    "df_metrics.to_csv('eval_metrics_1931_to_2008.csv')\n",
    "\n",
    "# Print true values and forecasts in table format\n",
    "df_true_values = pd.DataFrame(y_test, columns=['True Value'])\n",
    "df_forecasts = pd.DataFrame(model_data)\n",
    "\n",
    "print(\"True Values and Forecasts:\")\n",
    "print(pd.concat([df_true_values, df_forecasts], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3a74fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Smoothing (ETS) Results:\n",
      "RMSE: 3623.03337\n",
      "MAE: 3622.77098\n",
      "R-squared: -6903.21514\n",
      "SMAPE: 1.85874\n",
      "Training Time: 0.00757 seconds\n",
      "CPU Usage: 14.4 MHz\n",
      "Memory Used: 6840.1171875 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Storm\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:427: FutureWarning: After 0.13 initialization must be handled at model creation\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import io\n",
    "import requests\n",
    "import time\n",
    "import psutil\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values\n",
    "\n",
    "# Normalize the target variable\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#target_scaled = scaler.fit_transform(target.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size + test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Fit and evaluate Exponential Smoothing (ETS)\n",
    "ets_model = ExponentialSmoothing(X_train)\n",
    "start_time = time.time()\n",
    "ets_model_fit = ets_model.fit()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "ets_predictions = ets_model_fit.forecast(test_size)  # Forecast the same length as test data\n",
    "\n",
    "# Rescale the predictions back to the original scale\n",
    "predictions = scaler.inverse_transform(ets_predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(y_test, predictions[:-1])  # Exclude the last predicted value\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, predictions[:-1])\n",
    "r2 = r2_score(y_test, predictions[:-1])\n",
    "smape = 2 * np.mean(np.abs(y_test - predictions[:-1]) / (np.abs(y_test) + np.abs(predictions[:-1])))\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n",
    "print(f\"SMAPE: {smape:.5f}\")\n",
    "print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import io\n",
    "import requests\n",
    "import time\n",
    "import psutil\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from itertools import product\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values\n",
    "\n",
    "# Add a small positive value to handle non-positive values\n",
    "target += 1e-8\n",
    "\n",
    "# Normalize the target variable\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaled = scaler.fit_transform(target.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target_scaled[:train_size]\n",
    "test_data = target_scaled[train_size:train_size + test_size]\n",
    "\n",
    "# Split into input and output variables\n",
    "X_train = train_data[:-1]\n",
    "y_train = train_data[1:]\n",
    "X_test = test_data[:-1]\n",
    "y_test = test_data[1:]\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'trend': ['add', 'mul', None],\n",
    "    'seasonal': ['add', 'mul', None],\n",
    "    'seasonal_periods': [12],\n",
    "    'damped': [True, False]\n",
    "}\n",
    "\n",
    "# Define variables to store best model and its performance\n",
    "best_model = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "for params in product(*param_grid.values()):\n",
    "    model_params = dict(zip(param_grid.keys(), params))\n",
    "    model = ExponentialSmoothing(X_train, **model_params)\n",
    "    try:\n",
    "        model_fit = model.fit()\n",
    "        predictions = model_fit.forecast(test_size)\n",
    "        mse = mean_squared_error(y_test, predictions[:-1])\n",
    "        if mse < best_score:\n",
    "            best_model = model\n",
    "            best_score = mse\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Fit the best model to the training data\n",
    "start_time = time.time()\n",
    "best_model_fit = best_model.fit()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Forecast with the best model\n",
    "ets_predictions = best_model_fit.forecast(test_size)\n",
    "\n",
    "# Rescale the predictions back to the original scale\n",
    "predictions = scaler.inverse_transform(ets_predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(target[test_size:], predictions[:-1])  # Exclude the last predicted value\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(target[test_size:], predictions[:-1])\n",
    "r2 = r2_score(target[test_size:], predictions[:-1])\n",
    "smape = 2 * np.mean(np.abs(target[test_size:] - predictions[:-1]) / (np.abs(target[test_size:]) + np.abs(predictions[:-1])))\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(f\"Best Parameters: {best_model_fit.params}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n",
    "print(f\"SMAPE: {smape:.5f}\")\n",
    "print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f88da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Storm\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "C:\\Users\\Storm\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Smoothing (ETS) Results:\n",
      "RMSE: 120.52697\n",
      "MAE: 111.75694\n",
      "R-squared: -6.13097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Storm\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:427: FutureWarning: After 0.13 initialization must be handled at model creation\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size + test_size]\n",
    "\n",
    "# Fit the Exponential Smoothing model to the training data\n",
    "model = ExponentialSmoothing(train_data)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast using the trained model\n",
    "predictions = model_fit.forecast(test_size)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = ((predictions - test_data) ** 2).mean()\n",
    "rmse = mse ** 0.5\n",
    "mae = abs(predictions - test_data).mean()\n",
    "r2 = 1 - (mse / ((test_data - test_data.mean()) ** 2).mean())\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c114611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Storm\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:80: RuntimeWarning: overflow encountered in matmul\n",
      "  return err.T @ err\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Smoothing (ETS) Results:\n",
      "Best Parameters: trend=add, seasonal=add\n",
      "RMSE: 67.38787\n",
      "MAE: 54.06003\n",
      "R-squared: -1.22917\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size + test_size]\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'trend': ['add', 'mul', None],\n",
    "    'seasonal': ['add', 'mul', None],\n",
    "    'seasonal_periods': [12]\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "for trend in param_grid['trend']:\n",
    "    for seasonal in param_grid['seasonal']:\n",
    "        model = ExponentialSmoothing(train_data, seasonal_periods=12, trend=trend, seasonal=seasonal,\n",
    "                                     initialization_method='estimated')\n",
    "        model_fit = model.fit()\n",
    "        predictions = model_fit.forecast(test_size)\n",
    "        mse = mean_squared_error(test_data, predictions)\n",
    "        if mse < best_mse:\n",
    "            best_model = model_fit\n",
    "            best_mse = mse\n",
    "\n",
    "# Forecast using the best model\n",
    "predictions = best_model.forecast(test_size)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "rmse = np.sqrt(best_mse)\n",
    "mae = np.mean(np.abs(test_data - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data))\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(f\"Best Parameters: trend={best_model.model.trend}, seasonal={best_model.model.seasonal}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c1cfc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Smoothing (ETS) Results:\n",
      "Best Parameters: trend=add, seasonal=add\n",
      "RMSE: 67.39503\n",
      "MAE: 54.06286\n",
      "R-squared: -1.22964\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size + test_size]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data.reshape(-1, 1)).flatten()\n",
    "test_data_normalized = scaler.transform(test_data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'trend': ['add', None],\n",
    "    'seasonal': ['add', None],\n",
    "    'seasonal_periods': [12]\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "for trend in param_grid['trend']:\n",
    "    for seasonal in param_grid['seasonal']:\n",
    "        model = ExponentialSmoothing(train_data_normalized, seasonal_periods=12, trend=trend, seasonal=seasonal,\n",
    "                                     initialization_method='estimated')\n",
    "        model_fit = model.fit()\n",
    "        predictions_normalized = model_fit.forecast(test_size)\n",
    "        predictions = scaler.inverse_transform(predictions_normalized.reshape(-1, 1)).flatten()\n",
    "        mse = mean_squared_error(test_data, predictions)\n",
    "        if mse < best_mse:\n",
    "            best_model = model_fit\n",
    "            best_mse = mse\n",
    "\n",
    "# Forecast using the best model\n",
    "predictions_normalized = best_model.forecast(test_size)\n",
    "predictions = scaler.inverse_transform(predictions_normalized.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "rmse = np.sqrt(best_mse)\n",
    "mae = np.mean(np.abs(test_data - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data))\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(f\"Best Parameters: trend={best_model.model.trend}, seasonal={best_model.model.seasonal}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1947a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Smoothing (ETS) Results:\n",
      "Best Parameters: trend=add, seasonal=add\n",
      "RMSE: 67.39503\n",
      "MAE: 54.06286\n",
      "R-squared: -1.22964\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data['13-Month Smoothed Monthly Total Sunspot Number'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size + test_size]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data.reshape(-1, 1)).flatten()\n",
    "test_data_normalized = scaler.transform(test_data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'trend': ['add', None],\n",
    "    'seasonal': ['add', None],\n",
    "    'seasonal_periods': [12]\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "for trend in param_grid['trend']:\n",
    "    for seasonal in param_grid['seasonal']:\n",
    "        model = ExponentialSmoothing(\n",
    "            train_data_normalized,\n",
    "            seasonal_periods=12,\n",
    "            trend=trend,\n",
    "            seasonal=seasonal,\n",
    "            initialization_method='estimated',\n",
    "        )\n",
    "        model_fit = model.fit()\n",
    "        predictions_normalized = model_fit.forecast(test_size)\n",
    "        predictions = scaler.inverse_transform(predictions_normalized.reshape(-1, 1)).flatten()\n",
    "        mse = mean_squared_error(test_data, predictions)\n",
    "        if mse < best_mse:\n",
    "            best_model = model_fit\n",
    "            best_mse = mse\n",
    "\n",
    "# Forecast using the best model\n",
    "predictions_normalized = best_model.forecast(test_size)\n",
    "predictions = scaler.inverse_transform(predictions_normalized.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "rmse = np.sqrt(best_mse)\n",
    "mae = np.mean(np.abs(test_data - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data))\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(f\"Best Parameters: trend={best_model.model.trend}, seasonal={best_model.model.seasonal}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06ab370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prophet\n",
      "  Downloading prophet-1.1.4-py3-none-win_amd64.whl (12.9 MB)\n",
      "     -------------------------------------- 12.9/12.9 MB 195.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (4.62.3)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (3.6.3)\n",
      "Collecting cmdstanpy>=1.0.4\n",
      "  Downloading cmdstanpy-1.1.0-py3-none-any.whl (83 kB)\n",
      "     -------------------------------------- 83.2/83.2 kB 166.7 kB/s eta 0:00:00\n",
      "Collecting LunarCalendar>=0.0.9\n",
      "  Downloading LunarCalendar-0.0.9-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (2.8.2)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.0.0-py3-none-any.whl (31 kB)\n",
      "Collecting convertdate>=2.1.2\n",
      "  Downloading convertdate-2.4.0-py3-none-any.whl (47 kB)\n",
      "     -------------------------------------- 47.9/47.9 kB 240.0 kB/s eta 0:00:00\n",
      "Collecting holidays>=0.25\n",
      "  Downloading holidays-0.28-py3-none-any.whl (642 kB)\n",
      "     ------------------------------------ 642.9/642.9 kB 295.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=1.0.4 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (1.5.2)\n",
      "Collecting pymeeus<=1,>=0.3.13\n",
      "  Downloading PyMeeus-0.5.12.tar.gz (5.8 MB)\n",
      "     ---------------------------------------- 5.8/5.8 MB 204.5 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pytz in c:\\users\\storm\\anaconda3\\lib\\site-packages (from LunarCalendar>=0.0.9->prophet) (2022.7.1)\n",
      "Collecting ephem>=3.7.5.3\n",
      "  Downloading ephem-4.1.4-cp39-cp39-win_amd64.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 267.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (0.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (8.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.0->prophet) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\storm\\anaconda3\\lib\\site-packages (from tqdm>=4.36.1->prophet) (0.4.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from importlib-resources->prophet) (3.6.0)\n",
      "Building wheels for collected packages: pymeeus\n",
      "  Building wheel for pymeeus (setup.py): started\n",
      "  Building wheel for pymeeus (setup.py): finished with status 'done'\n",
      "  Created wheel for pymeeus: filename=PyMeeus-0.5.12-py3-none-any.whl size=732018 sha256=85c099ba1c0a2f43e6889cace92a4917818102fa88d2a2c1e47b8cb41fc60738\n",
      "  Stored in directory: c:\\users\\storm\\appdata\\local\\pip\\cache\\wheels\\04\\1f\\e5\\8dd0c661cd8d252817655dc14a84f7ae045d6616594145aa81\n",
      "Successfully built pymeeus\n",
      "Installing collected packages: pymeeus, ephem, importlib-resources, convertdate, LunarCalendar, holidays, cmdstanpy, prophet\n",
      "Successfully installed LunarCalendar-0.0.9 cmdstanpy-1.1.0 convertdate-2.4.0 ephem-4.1.4 holidays-0.28 importlib-resources-6.0.0 prophet-1.1.4 pymeeus-0.5.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a7c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Storm\\AppData\\Local\\Temp/ipykernel_13384/3267026952.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['y'] = scaler.fit_transform(train_data['y'].values.reshape(-1, 1)).flatten()\n",
      "C:\\Users\\Storm\\AppData\\Local\\Temp/ipykernel_13384/3267026952.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['y'] = scaler.transform(test_data['y'].values.reshape(-1, 1)).flatten()\n",
      "18:46:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:24 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "18:46:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:26 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "18:46:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:28 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "18:46:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:29 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "18:46:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:34 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import io\n",
    "import requests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data[['Year', 'Month', '13-Month Smoothed Monthly Total Sunspot Number']]\n",
    "target.columns = ['ds', 'month', 'y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size]\n",
    "test_data = target[train_size:train_size + test_size]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data['y'] = scaler.fit_transform(train_data['y'].values.reshape(-1, 1)).flatten()\n",
    "test_data['y'] = scaler.transform(test_data['y'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'seasonality_mode': ['additive', 'multiplicative'],\n",
    "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 1],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1, 10],\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "for seasonality_mode in param_grid['seasonality_mode']:\n",
    "    for changepoint_prior_scale in param_grid['changepoint_prior_scale']:\n",
    "        for seasonality_prior_scale in param_grid['seasonality_prior_scale']:\n",
    "            model = Prophet(\n",
    "                seasonality_mode=seasonality_mode,\n",
    "                changepoint_prior_scale=changepoint_prior_scale,\n",
    "                seasonality_prior_scale=seasonality_prior_scale,\n",
    "            )\n",
    "            model.fit(train_data)\n",
    "            future = model.make_future_dataframe(periods=test_size, freq='M')\n",
    "            forecast = model.predict(future)\n",
    "            predictions = forecast['yhat'].tail(test_size).values\n",
    "            mse = mean_squared_error(test_data['y'], predictions)\n",
    "            if mse < best_mse:\n",
    "                best_model = model\n",
    "                best_mse = mse\n",
    "\n",
    "# Forecast using the best model\n",
    "future = best_model.make_future_dataframe(periods=test_size, freq='M')\n",
    "forecast = best_model.predict(future)\n",
    "predictions = forecast['yhat'].tail(test_size).values\n",
    "predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "rmse = np.sqrt(best_mse)\n",
    "mae = np.mean(np.abs(test_data['y'] - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data['y']))\n",
    "\n",
    "# Print the results\n",
    "print(\"Prophet Results:\")\n",
    "print(f\"Best Parameters: seasonality_mode={best_model.seasonality_mode}, \"\n",
    "      f\"changepoint_prior_scale={best_model.changepoint_prior_scale}, \"\n",
    "      f\"seasonality_prior_scale={best_model.seasonality_prior_scale}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b8574f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.15.0-py2.py3-none-any.whl (15.5 MB)\n",
      "     --------------------------------------- 15.5/15.5 MB 70.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\storm\\anaconda3\\lib\\site-packages (from plotly) (23.0)\n",
      "Installing collected packages: plotly\n",
      "Successfully installed plotly-5.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc33c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:05:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:12 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:05:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:14 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:05:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:15 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:05:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:17 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:05:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:21 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:05:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:23 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:05:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:24 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:05:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:25 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:05:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:39 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet Results:\n",
      "Best Parameters:\n",
      "changepoint_prior_scale: 0.01\n",
      "seasonality_prior_scale: 10\n",
      "seasonality_mode: multiplicative\n",
      "RMSE: 0.37454\n",
      "MAE: 89.63376\n",
      "R-squared: -1.06572\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from prophet import Prophet\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data[['Year', 'Month', '13-Month Smoothed Monthly Total Sunspot Number']]\n",
    "target.columns = ['ds', 'Month', 'y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size].copy()\n",
    "test_data = target[train_size:train_size + test_size].copy()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data['y'] = scaler.fit_transform(train_data['y'].values.reshape(-1, 1)).flatten()\n",
    "test_data['y'] = scaler.transform(test_data['y'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 1],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1, 10],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "for cp_scale in param_grid['changepoint_prior_scale']:\n",
    "    for season_scale in param_grid['seasonality_prior_scale']:\n",
    "        for season_mode in param_grid['seasonality_mode']:\n",
    "            model = Prophet(\n",
    "                growth='linear',\n",
    "                seasonality_mode=season_mode,\n",
    "                changepoint_prior_scale=cp_scale,\n",
    "                seasonality_prior_scale=season_scale,\n",
    "                holidays=None,\n",
    "                daily_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                yearly_seasonality=False,\n",
    "                interval_width=0.95,\n",
    "            )\n",
    "            model.add_seasonality(name='yearly', period=365.25, fourier_order=10)\n",
    "            model.fit(train_data)\n",
    "            future = model.make_future_dataframe(periods=test_size, freq='M')\n",
    "            forecast = model.predict(future)\n",
    "            predictions = forecast['yhat'][-test_size:].values\n",
    "            mse = mean_squared_error(test_data['y'], predictions)\n",
    "            if mse < best_mse:\n",
    "                best_model = model\n",
    "                best_mse = mse\n",
    "                best_params = {\n",
    "                    'changepoint_prior_scale': cp_scale,\n",
    "                    'seasonality_prior_scale': season_scale,\n",
    "                    'seasonality_mode': season_mode\n",
    "                }\n",
    "\n",
    "# Forecast using the best model\n",
    "future = best_model.make_future_dataframe(periods=test_size, freq='M')\n",
    "forecast = best_model.predict(future)\n",
    "predictions = forecast['yhat'][-test_size:].values\n",
    "predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "rmse = np.sqrt(best_mse)\n",
    "mae = np.mean(np.abs(test_data['y'] - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data['y']))\n",
    "\n",
    "# Print the results\n",
    "print(\"Prophet Results:\")\n",
    "print(\"Best Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7faee115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:15:30 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:15:31 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:15:33 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:15:34 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:15:36 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:15:37 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:15:38 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:15:40 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet Results:\n",
      "Best Parameters:\n",
      "cp_scale: 0.01\n",
      "season_scale: 1\n",
      "season_mode: multiplicative\n",
      "RMSE: 67.93616\n",
      "MAE: 59.59697\n",
      "R-squared: -1.26559\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "# Set the logging level of cmdstanpy to suppress log messages\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data[['Year', 'Month', '13-Month Smoothed Monthly Total Sunspot Number']]\n",
    "target.columns = ['ds', 'Month', 'y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size].copy()\n",
    "test_data = target[train_size:train_size + test_size].copy()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'cp_scale': [0.001, 0.01, 0.1, 1],\n",
    "    'season_scale': [0.01, 0.1, 1, 10],\n",
    "    'season_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "for cp_scale in param_grid['cp_scale']:\n",
    "    for season_scale in param_grid['season_scale']:\n",
    "        for season_mode in param_grid['season_mode']:\n",
    "            model = Prophet(\n",
    "                growth='linear',\n",
    "                seasonality_mode=season_mode,\n",
    "                changepoint_prior_scale=cp_scale,\n",
    "                seasonality_prior_scale=season_scale,\n",
    "                holidays=None,\n",
    "                daily_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                yearly_seasonality=False,\n",
    "                interval_width=0.95,\n",
    "            )\n",
    "            model.add_seasonality(name='yearly', period=365.25, fourier_order=10)\n",
    "            model.fit(train_data)\n",
    "            future = model.make_future_dataframe(periods=test_size, freq='M')\n",
    "            forecast = model.predict(future)\n",
    "            predictions = forecast['yhat'][-test_size:].values\n",
    "            mse = mean_squared_error(test_data['y'], predictions)\n",
    "            if mse < best_mse:\n",
    "                best_model = model\n",
    "                best_mse = mse\n",
    "                best_params = {\n",
    "                    'cp_scale': cp_scale,\n",
    "                    'season_scale': season_scale,\n",
    "                    'season_mode': season_mode\n",
    "                }\n",
    "\n",
    "# Forecast using the best model\n",
    "future = best_model.make_future_dataframe(periods=test_size, freq='M')\n",
    "forecast = best_model.predict(future)\n",
    "predictions = forecast['yhat'][-test_size:].values\n",
    "\n",
    "# Calculate the evaluation metrics without scaling\n",
    "rmse = np.sqrt(mean_squared_error(test_data['y'], predictions))\n",
    "mae = np.mean(np.abs(test_data['y'] - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data['y']))\n",
    "\n",
    "# Print the results\n",
    "print(\"Prophet Results:\")\n",
    "print(\"Best Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2316aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prophet in c:\\users\\storm\\anaconda3\\lib\\site-packages (1.1.4)\n",
      "Requirement already satisfied: LunarCalendar>=0.0.9 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (0.0.9)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (6.0.0)\n",
      "Requirement already satisfied: holidays>=0.25 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (0.28)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.0.4 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (1.22.4)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (3.6.3)\n",
      "Requirement already satisfied: convertdate>=2.1.2 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (2.4.0)\n",
      "Requirement already satisfied: cmdstanpy>=1.0.4 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (1.1.0)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from prophet) (4.62.3)\n",
      "Requirement already satisfied: pymeeus<=1,>=0.3.13 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from convertdate>=2.1.2->prophet) (0.5.12)\n",
      "Requirement already satisfied: ephem>=3.7.5.3 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from LunarCalendar>=0.0.9->prophet) (4.1.4)\n",
      "Requirement already satisfied: pytz in c:\\users\\storm\\anaconda3\\lib\\site-packages (from LunarCalendar>=0.0.9->prophet) (2022.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (8.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.0->prophet) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\storm\\anaconda3\\lib\\site-packages (from tqdm>=4.36.1->prophet) (0.4.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\storm\\anaconda3\\lib\\site-packages (from importlib-resources->prophet) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install fbprophet\n",
    "!pip install prophet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f05cdabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\storm\\anaconda3\\lib\\site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.2-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 70.3 kB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\Storm\\anaconda3\\python.exe -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\storm\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55d94093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:20:46 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:20:47 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:20:48 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:20:49 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:20:50 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:20:52 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:20:53 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:20:55 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Smoothing (ETS) Results:\n",
      "Best Parameters:\n",
      "changepoint_prior_scale: 0.01\n",
      "seasonality_prior_scale: 1\n",
      "seasonality_mode: multiplicative\n",
      "RMSE: 67.93616\n",
      "MAE: 59.59697\n",
      "R-squared: -1.26559\n",
      "SMAPE: 51.30096\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "# Set the logging level of cmdstanpy to suppress log messages\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data[['Year', 'Month', '13-Month Smoothed Monthly Total Sunspot Number']]\n",
    "target.columns = ['ds', 'Month', 'y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size].copy()\n",
    "test_data = target[train_size:train_size + test_size].copy()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 1],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1, 10],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "for cp_scale in param_grid['changepoint_prior_scale']:\n",
    "    for season_scale in param_grid['seasonality_prior_scale']:\n",
    "        for season_mode in param_grid['seasonality_mode']:\n",
    "            model = Prophet(\n",
    "                growth='linear',\n",
    "                seasonality_mode=season_mode,\n",
    "                changepoint_prior_scale=cp_scale,\n",
    "                seasonality_prior_scale=season_scale,\n",
    "                holidays=None,\n",
    "                daily_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                yearly_seasonality=False,\n",
    "                interval_width=0.95,\n",
    "            )\n",
    "            model.add_seasonality(name='yearly', period=365.25, fourier_order=10)\n",
    "            model.fit(train_data)\n",
    "            future = model.make_future_dataframe(periods=test_size, freq='M')\n",
    "            forecast = model.predict(future)\n",
    "            predictions = forecast['yhat'][-test_size:].values\n",
    "            mse = mean_squared_error(test_data['y'], predictions)\n",
    "            if mse < best_mse:\n",
    "                best_model = model\n",
    "                best_mse = mse\n",
    "                best_params = {\n",
    "                    'changepoint_prior_scale': cp_scale,\n",
    "                    'seasonality_prior_scale': season_scale,\n",
    "                    'seasonality_mode': season_mode\n",
    "                }\n",
    "\n",
    "# Forecast using the best model\n",
    "future = best_model.make_future_dataframe(periods=test_size, freq='M')\n",
    "forecast = best_model.predict(future)\n",
    "predictions = forecast['yhat'][-test_size:].values\n",
    "\n",
    "# Calculate the evaluation metrics without scaling\n",
    "rmse = np.sqrt(mean_squared_error(test_data['y'], predictions))\n",
    "mae = np.mean(np.abs(test_data['y'] - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data['y']))\n",
    "\n",
    "# Calculate SMAPE\n",
    "def smape(actual, forecast):\n",
    "    return np.mean((np.abs(actual - forecast) / (np.abs(actual) + np.abs(forecast))) * 200)\n",
    "\n",
    "smape_value = smape(test_data['y'], predictions)\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(\"Best Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n",
    "print(f\"SMAPE: {smape_value:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c59f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:23:32 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:23:33 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:23:35 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:23:36 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:23:37 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:23:39 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:23:40 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "19:23:41 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Smoothing (ETS) Results:\n",
      "Best Parameters: {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 1, 'seasonality_mode': 'multiplicative'}\n",
      "RMSE: 67.93616\n",
      "MAE: 59.59697\n",
      "R-squared: -1.26559\n",
      "SMAPE: 51.30096\n",
      "Training Time: 25.57862 seconds\n",
      "CPU Usage: 17.1 MHz\n",
      "Memory Used: 7342.16796875 MB\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Set the logging level of cmdstanpy to suppress log messages\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1875) & (data['Year'] <= 1941)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target = filtered_data[['Year', 'Month', '13-Month Smoothed Monthly Total Sunspot Number']]\n",
    "target.columns = ['ds', 'Month', 'y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 720  # Number of months for training\n",
    "test_size = 72  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size].copy()\n",
    "test_data = target[train_size:train_size + test_size].copy()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 1],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1, 10],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "start_time = time.time()\n",
    "for cp_scale in param_grid['changepoint_prior_scale']:\n",
    "    for season_scale in param_grid['seasonality_prior_scale']:\n",
    "        for season_mode in param_grid['seasonality_mode']:\n",
    "            model = Prophet(\n",
    "                growth='linear',\n",
    "                seasonality_mode=season_mode,\n",
    "                changepoint_prior_scale=cp_scale,\n",
    "                seasonality_prior_scale=season_scale,\n",
    "                holidays=None,\n",
    "                daily_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                yearly_seasonality=False,\n",
    "                interval_width=0.95,\n",
    "            )\n",
    "            model.add_seasonality(name='yearly', period=365.25, fourier_order=10)\n",
    "            model.fit(train_data)\n",
    "            future = model.make_future_dataframe(periods=test_size, freq='M')\n",
    "            forecast = model.predict(future)\n",
    "            predictions = forecast['yhat'][-test_size:].values\n",
    "            mse = mean_squared_error(test_data['y'], predictions)\n",
    "            if mse < best_mse:\n",
    "                best_model = model\n",
    "                best_mse = mse\n",
    "                best_params = {\n",
    "                    'changepoint_prior_scale': cp_scale,\n",
    "                    'seasonality_prior_scale': season_scale,\n",
    "                    'seasonality_mode': season_mode\n",
    "                }\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Forecast using the best model\n",
    "future = best_model.make_future_dataframe(periods=test_size, freq='M')\n",
    "forecast = best_model.predict(future)\n",
    "predictions = forecast['yhat'][-test_size:].values\n",
    "\n",
    "# Calculate the evaluation metrics without scaling\n",
    "rmse = np.sqrt(mean_squared_error(test_data['y'], predictions))\n",
    "mae = np.mean(np.abs(test_data['y'] - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data['y']))\n",
    "\n",
    "# Calculate SMAPE\n",
    "def smape(actual, forecast):\n",
    "    return np.mean((np.abs(actual - forecast) / (np.abs(actual) + np.abs(forecast))) * 200)\n",
    "\n",
    "smape_value = smape(test_data['y'], predictions)\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n",
    "print(f\"SMAPE: {smape_value:.5f}\")\n",
    "print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f8740d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:18:10 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "21:18:11 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "21:18:13 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "21:18:15 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Smoothing (ETS) Results:\n",
      "Best Parameters: {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 0.1, 'seasonality_mode': 'additive'}\n",
      "RMSE: 34.95585\n",
      "MAE: 31.94644\n",
      "R-squared: -0.06092\n",
      "SMAPE: 41.29613\n",
      "Training Time: 17.77968 seconds\n",
      "CPU Usage: 23.8 MHz\n",
      "Memory Used: 6943.9921875 MB\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Set the logging level of cmdstanpy to suppress log messages\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1854) & (data['Year'] <= 1931)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target_column = '13-Month Smoothed Monthly Total Sunspot Number'\n",
    "target = filtered_data[['Year', 'Month', target_column]]\n",
    "target.columns = ['ds', 'Month', 'y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 840  # Number of months for training\n",
    "test_size = 84  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size].copy()\n",
    "test_data = target[train_size:train_size + test_size].copy()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 1],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1, 10],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "start_time = time.time()\n",
    "for cp_scale in param_grid['changepoint_prior_scale']:\n",
    "    for season_scale in param_grid['seasonality_prior_scale']:\n",
    "        for season_mode in param_grid['seasonality_mode']:\n",
    "            model = Prophet(\n",
    "                growth='linear',\n",
    "                seasonality_mode=season_mode,\n",
    "                changepoint_prior_scale=cp_scale,\n",
    "                seasonality_prior_scale=season_scale,\n",
    "                holidays=None,\n",
    "                daily_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                yearly_seasonality=False,\n",
    "                interval_width=0.95,\n",
    "            )\n",
    "            model.add_seasonality(name='yearly', period=365.25, fourier_order=10)\n",
    "            model.fit(train_data)\n",
    "            future = model.make_future_dataframe(periods=test_size, freq='M')\n",
    "            forecast = model.predict(future)\n",
    "            predictions = forecast['yhat'][-test_size:].values\n",
    "            mse = mean_squared_error(test_data['y'], predictions)\n",
    "            if mse < best_mse:\n",
    "                best_model = model\n",
    "                best_mse = mse\n",
    "                best_params = {\n",
    "                    'changepoint_prior_scale': cp_scale,\n",
    "                    'seasonality_prior_scale': season_scale,\n",
    "                    'seasonality_mode': season_mode\n",
    "                }\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Forecast using the best model\n",
    "future = best_model.make_future_dataframe(periods=test_size, freq='M')\n",
    "forecast = best_model.predict(future)\n",
    "predictions = forecast['yhat'][-test_size:].values\n",
    "\n",
    "# Calculate the evaluation metrics without scaling\n",
    "rmse = np.sqrt(mean_squared_error(test_data['y'], predictions))\n",
    "mae = np.mean(np.abs(test_data['y'] - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data['y']))\n",
    "\n",
    "# Calculate SMAPE\n",
    "def smape(actual, forecast):\n",
    "    return np.mean((np.abs(actual - forecast) / (np.abs(actual) + np.abs(forecast))) * 200)\n",
    "\n",
    "smape_value = smape(test_data['y'], predictions)\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n",
    "print(f\"SMAPE: {smape_value:.5f}\")\n",
    "print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61b1da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Smoothing (ETS) Results:\n",
      "Best Parameters: {'changepoint_prior_scale': 1, 'seasonality_prior_scale': 0.01, 'seasonality_mode': 'additive'}\n",
      "RMSE: 43.00714\n",
      "MAE: 37.29695\n",
      "R-squared: 0.46557\n",
      "SMAPE: 58.73349\n",
      "Training Time: 13.69575 seconds\n",
      "CPU Usage: 31.0 MHz\n",
      "Memory Used: 6976.8515625 MB\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Set the logging level of cmdstanpy to suppress log messages\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "# Fetch the data from the website\n",
    "url = 'https://www.sidc.be/SILSO/INFO/snmstotcsv.php'\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(io.StringIO(response.text), delimiter=';', header=None)\n",
    "data.columns = ['Year', 'Month', 'Decimal date', '13-Month Smoothed Monthly Total Sunspot Number',\n",
    "                'Standard deviation', 'Number of observations', 'Definitive/Provisional']\n",
    "\n",
    "# Filter the data for the desired interval (1875 to 1941)\n",
    "filtered_data = data[(data['Year'] >= 1931) & (data['Year'] <= 2008)].copy()\n",
    "\n",
    "# Select the '13-Month Smoothed Monthly Total Sunspot Number' column as the target variable\n",
    "target_column = '13-Month Smoothed Monthly Total Sunspot Number'\n",
    "target = filtered_data[['Year', 'Month', target_column]]\n",
    "target.columns = ['ds', 'Month', 'y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = 840  # Number of months for training\n",
    "test_size = 84  # Number of months for testing\n",
    "\n",
    "train_data = target[:train_size].copy()\n",
    "test_data = target[train_size:train_size + test_size].copy()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 1],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1, 10],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Initialize variables to store best model and its performance\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Perform manual hyperparameter tuning\n",
    "start_time = time.time()\n",
    "for cp_scale in param_grid['changepoint_prior_scale']:\n",
    "    for season_scale in param_grid['seasonality_prior_scale']:\n",
    "        for season_mode in param_grid['seasonality_mode']:\n",
    "            model = Prophet(\n",
    "                growth='linear',\n",
    "                seasonality_mode=season_mode,\n",
    "                changepoint_prior_scale=cp_scale,\n",
    "                seasonality_prior_scale=season_scale,\n",
    "                holidays=None,\n",
    "                daily_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                yearly_seasonality=False,\n",
    "                interval_width=0.95,\n",
    "            )\n",
    "            model.add_seasonality(name='yearly', period=365.25, fourier_order=10)\n",
    "            model.fit(train_data)\n",
    "            future = model.make_future_dataframe(periods=test_size, freq='M')\n",
    "            forecast = model.predict(future)\n",
    "            predictions = forecast['yhat'][-test_size:].values\n",
    "            mse = mean_squared_error(test_data['y'], predictions)\n",
    "            if mse < best_mse:\n",
    "                best_model = model\n",
    "                best_mse = mse\n",
    "                best_params = {\n",
    "                    'changepoint_prior_scale': cp_scale,\n",
    "                    'seasonality_prior_scale': season_scale,\n",
    "                    'seasonality_mode': season_mode\n",
    "                }\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Forecast using the best model\n",
    "future = best_model.make_future_dataframe(periods=test_size, freq='M')\n",
    "forecast = best_model.predict(future)\n",
    "predictions = forecast['yhat'][-test_size:].values\n",
    "\n",
    "# Calculate the evaluation metrics without scaling\n",
    "rmse = np.sqrt(mean_squared_error(test_data['y'], predictions))\n",
    "mae = np.mean(np.abs(test_data['y'] - predictions))\n",
    "r2 = 1 - (best_mse / np.var(test_data['y']))\n",
    "\n",
    "# Calculate SMAPE\n",
    "def smape(actual, forecast):\n",
    "    return np.mean((np.abs(actual - forecast) / (np.abs(actual) + np.abs(forecast))) * 200)\n",
    "\n",
    "smape_value = smape(test_data['y'], predictions)\n",
    "\n",
    "# Print the results\n",
    "print(\"Exponential Smoothing (ETS) Results:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"RMSE: {rmse:.5f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")\n",
    "print(f\"SMAPE: {smape_value:.5f}\")\n",
    "print(f\"Training Time: {training_time:.5f} seconds\")\n",
    "print(f\"CPU Usage: {psutil.cpu_percent()} MHz\")\n",
    "print(f\"Memory Used: {psutil.virtual_memory().used / 1024 / 1024} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126866ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(y_train) + 1), y_train, label='Actual (Training)')\n",
    "plt.plot(range(len(y_train), len(y_train) + len(y_test)), y_test, label='Actual (Testing)')\n",
    "plt.plot(range(1, len(y_train) + 1), train_predictions, label='Predicted (Training)')\n",
    "plt.plot(range(len(y_train), len(y_train) + len(y_test)), test_predictions, label='Predicted (Testing)')\n",
    "plt.title('ETS Model Performance')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Sunspot Number')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2eb77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
